{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98da5ac3",
   "metadata": {},
   "source": [
    "# The MS COCO classification challenge\n",
    "\n",
    "Razmig Kéchichian\n",
    "\n",
    "This notebook defines the multi-class classification challenge on the [MS COCO dataset](https://cocodataset.org/). It defines the problem, sets the rules of organization and presents tools you are provided with to accomplish the challenge.\n",
    "\n",
    "\n",
    "## 1. Problem statement\n",
    "\n",
    "Each image has **several** categories of objects to predict, hence the difference compared to the classification problem we have seen on the CIFAR10 dataset where each image belonged to a **single** category, therefore the network loss function and prediction mechanism (only highest output probability) were defined taking this constraint into account.\n",
    "\n",
    "We adapted the MS COCO dataset for the requirements of this challenge by, among other things, reducing the number of images and their dimensions to facilitate processing.\n",
    "\n",
    "In the companion `ms-coco.zip` compressed directory you will find two sub-directories:\n",
    "- `images`: which contains the images in train (65k) and test (~5k) subsets,\n",
    "- `labels`: which lists labels for each of the images in the train subset only.\n",
    "\n",
    "Each label file gives a list of class IDs that correspond to the class index in the following tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \n",
    "           \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
    "           \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",       \n",
    "           \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "           \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "           \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \n",
    "           \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \n",
    "           \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \n",
    "           \"hair drier\", \"toothbrush\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf52f93",
   "metadata": {},
   "source": [
    "Your goal is to follow a **transfer learning strategy** in training and validating a network on **your own distribution of training data into training and a validation subsets**, then to **test it on the test subset** by producing a [JSON file](https://en.wikipedia.org/wiki/JSON) with content of the following format:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"000000000139\": [\n",
    "        56,\n",
    "        60,\n",
    "        62\n",
    "    ],\n",
    "    \"000000000285\": [\n",
    "        21,\n",
    "    ],\n",
    "    \"000000000632\": [\n",
    "        57,\n",
    "        59,\n",
    "    73\n",
    "    ],\n",
    "    # other test images\n",
    "}\n",
    "```\n",
    "\n",
    "In this file, the name (without extension) of each test image is associated with a list of class indices predicted by your network. Make sure that the JSON file you produce **follows this format strictly**.\n",
    "\n",
    "You will submit your JSON prediction file to the following [online evaluation server and leaderboard](https://www.creatis.insa-lyon.fr/kechichian/ms-coco-classif-leaderboard.html), which will evaluate your predictions on test set labels, unavailable to you.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>WARNING:</b> Use this server with <b>the greatest care</b>. A new submission with identical Participant or group name will <b>overwrite</b> the identically named submission, if one already exists, therefore check the leaderboard first. <b>Do not make duplicate leaderboard entries for your group</b>, keep track of your test scores privately. Also pay attention to upload only JSON files of the required format.<br>\n",
    "</div>\n",
    "\n",
    "The evaluation server calculates and returns mean performances over all classes, and optionally per class performances. Entries in the leaderboard are sorted by the F1 metric.\n",
    "\n",
    "You can request an evaluation as many times as you want. It is up to you to specify the final evaluation by updating the leaderboard entry corresponding to your Participant or group name. This entry will be taken into account for grading your work.\n",
    "\n",
    "It goes without saying that it is **prohibited** to use another distribution of the MS COCO database for training, e.g. the Torchvision dataset.\n",
    "\n",
    "\n",
    "## 2. Organization\n",
    "\n",
    "- Given the scope of the project, you will work in groups of 2. \n",
    "- Work on the challenge begins on IAV lab 3 session, that is on the **23rd of September**.\n",
    "- Results are due 10 days later, that is on the **3rd of October, 18:00**. They comrpise:\n",
    "    - a submission to the leaderboard,\n",
    "    - a commented Python script (with any necessary modules) or Jupyter Notebook, uploaded on Moodle in the challenge repository by one of the members of the group.\n",
    "    \n",
    "    \n",
    "## 3. Tools\n",
    "\n",
    "In addition to the MS COCO annotated data and the evaluation server, we provide you with most code building blocks. Your task is to understand them and use them to create the glue logic, that is the main program, putting all these blocks together and completing them as necessary to implement a complete machine learning workflow to train and validate a model, and produce the test JSON file.\n",
    "\n",
    "### 3.1 Custom `Dataset`s\n",
    "\n",
    "We provide you with two custom `torch.utils.data.Dataset` sub-classes to use in training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class COCOTrainImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, annotations_dir, max_images=None, transform=None):\n",
    "        self.img_labels = sorted(glob(\"*.cls\", root_dir=annotations_dir))\n",
    "        if max_images:\n",
    "            self.img_labels = self.img_labels[:max_images]\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, Path(self.img_labels[idx]).stem + \".jpg\")\n",
    "        labels_path = os.path.join(self.annotations_dir, self.img_labels[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        with open(labels_path) as f: \n",
    "            labels = [int(label) for label in f.readlines()]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        labels = torch.zeros(80).scatter_(0, torch.tensor(labels), value=1)\n",
    "        return image, labels\n",
    "\n",
    "\n",
    "class COCOTestImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_list = sorted(glob(\"*.jpg\", root_dir=img_dir))    \n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_list[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, Path(img_path).stem # filename w/o extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d805e2",
   "metadata": {},
   "source": [
    "### 3.2 Training and validation loops\n",
    "\n",
    "The following are two general-purpose classification train and validation loop functions to be called inside the epochs for-loop with appropriate argument settings.\n",
    "\n",
    "Pay particular attention to the `validation_loop()` function's arguments `multi_task`, `th_multi_task` and `one_hot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb693462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def train_loop(train_loader, net, criterion, optimizer, device,\n",
    "               mbatch_loss_group=-1):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    mbatch_losses = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # following condition False by default, unless mbatch_loss_group > 0\n",
    "        if i % mbatch_loss_group == mbatch_loss_group - 1:\n",
    "            mbatch_losses.append(running_loss / mbatch_loss_group)\n",
    "            running_loss = 0.0\n",
    "    if mbatch_loss_group > 0:\n",
    "        return mbatch_losses\n",
    "\n",
    "\n",
    "def validation_loop(val_loader, net, criterion, num_classes, device,\n",
    "                    multi_task=False, th_multi_task=0.5, one_hot=False, class_metrics=False):\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    size = len(val_loader.dataset)\n",
    "    class_total = {label:0 for label in range(num_classes)}\n",
    "    class_tp = {label:0 for label in range(num_classes)}\n",
    "    class_fp = {label:0 for label in range(num_classes)}\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item() * images.size(0)\n",
    "            if not multi_task:    \n",
    "                predictions = torch.zeros_like(outputs)\n",
    "                predictions[torch.arange(outputs.shape[0]), torch.argmax(outputs, dim=1)] = 1.0\n",
    "            else:\n",
    "                predictions = torch.where(outputs > th_multi_task, 1.0, 0.0)\n",
    "            if not one_hot:\n",
    "                labels_mat = torch.zeros_like(outputs)\n",
    "                labels_mat[torch.arange(outputs.shape[0]), labels] = 1.0\n",
    "                labels = labels_mat\n",
    "                \n",
    "            tps = predictions * labels\n",
    "            fps = predictions - tps\n",
    "            \n",
    "            tps = tps.sum(dim=0)\n",
    "            fps = fps.sum(dim=0)\n",
    "            lbls = labels.sum(dim=0)  \n",
    "                \n",
    "            for c in range(num_classes):\n",
    "                class_tp[c] += tps[c]\n",
    "                class_fp[c] += fps[c]\n",
    "                class_total[c] += lbls[c]\n",
    "                    \n",
    "            correct += tps.sum()\n",
    "\n",
    "    class_prec = []\n",
    "    class_recall = []\n",
    "    freqs = []\n",
    "    for c in range(num_classes):\n",
    "        class_prec.append(0 if class_tp[c] == 0 else\n",
    "                          class_tp[c] / (class_tp[c] + class_fp[c]))\n",
    "        class_recall.append(0 if class_tp[c] == 0 else\n",
    "                            class_tp[c] / class_total[c])\n",
    "        freqs.append(class_total[c])\n",
    "\n",
    "    freqs = torch.tensor(freqs)\n",
    "    class_weights = 1. / freqs\n",
    "    class_weights /= class_weights.sum()\n",
    "    class_prec = torch.tensor(class_prec)\n",
    "    class_recall = torch.tensor(class_recall)\n",
    "    prec = (class_prec * class_weights).sum()\n",
    "    recall = (class_recall * class_weights).sum()\n",
    "    f1 = 2. / (1/prec + 1/recall)\n",
    "    val_loss = loss / size\n",
    "    accuracy = correct / freqs.sum()\n",
    "    results = {\"loss\": val_loss, \"accuracy\": accuracy, \"f1\": f1,\\\n",
    "               \"precision\": prec, \"recall\": recall}\n",
    "\n",
    "    if class_metrics:\n",
    "        class_results = []\n",
    "        for p, r in zip(class_prec, class_recall):\n",
    "            f1 = (0 if p == r == 0 else 2. / (1/p + 1/r))\n",
    "            class_results.append({\"f1\": f1, \"precision\": p, \"recall\": r})\n",
    "        results = results, class_results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d8e84",
   "metadata": {},
   "source": [
    "### 3.3 Tensorboard logging (optional)\n",
    "\n",
    "Evaluation metrics and losses produced by the `validation_loop()` function on train and validation data can be logged to a [Tensorboard `SummaryWriter`](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) which allows you to observe training graphically via the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graphs(summary_writer, epoch, train_results, test_results,\n",
    "                  train_class_results=None, test_class_results=None, \n",
    "                  class_names = None, mbatch_group=-1, mbatch_count=0, mbatch_losses=None):\n",
    "    if mbatch_group > 0:\n",
    "        for i in range(len(mbatch_losses)):\n",
    "            summary_writer.add_scalar(\"Losses/Train mini-batches\",\n",
    "                                  mbatch_losses[i],\n",
    "                                  epoch * mbatch_count + (i+1)*mbatch_group)\n",
    "\n",
    "    summary_writer.add_scalars(\"Losses/Train Loss vs Test Loss\",\n",
    "                               {\"Train Loss\" : train_results[\"loss\"],\n",
    "                                \"Test Loss\" : test_results[\"loss\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Accuracy vs Test Accuracy\",\n",
    "                               {\"Train Accuracy\" : train_results[\"accuracy\"],\n",
    "                                \"Test Accuracy\" : test_results[\"accuracy\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train F1 vs Test F1\",\n",
    "                               {\"Train F1\" : train_results[\"f1\"],\n",
    "                                \"Test F1\" : test_results[\"f1\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Precision vs Test Precision\",\n",
    "                               {\"Train Precision\" : train_results[\"precision\"],\n",
    "                                \"Test Precision\" : test_results[\"precision\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Recall vs Test Recall\",\n",
    "                               {\"Train Recall\" : train_results[\"recall\"],\n",
    "                                \"Test Recall\" : test_results[\"recall\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    if train_class_results and test_class_results:\n",
    "        for i in range(len(train_class_results)):\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train F1 vs Test F1\",\n",
    "                                       {\"Train F1\" : train_class_results[i][\"f1\"],\n",
    "                                        \"Test F1\" : test_class_results[i][\"f1\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train Precision vs Test Precision\",\n",
    "                                       {\"Train Precision\" : train_class_results[i][\"precision\"],\n",
    "                                        \"Test Precision\" : test_class_results[i][\"precision\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train Recall vs Test Recall\",\n",
    "                                       {\"Train Recall\" : train_class_results[i][\"recall\"],\n",
    "                                        \"Test Recall\" : test_class_results[i][\"recall\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "    summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af63e9",
   "metadata": {},
   "source": [
    "## 4. The skeleton of the model training and validation program\n",
    "\n",
    "Your main program should have more or less the following sections and control flow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde75c9",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9350df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\"\"\"\n",
    "Configuration for COCO multi-label classification training.\n",
    "Defines model type, preprocessing, optimizer, and training hyperparameters.\n",
    "Specifies dataset directories and batch/data loader settings.\n",
    "Includes validation split and threshold for multi-label predictions.\n",
    "\"\"\"\n",
    "\n",
    "CONFIG = {\n",
    "    \"model\": \"mobilenet_v3_small\",  # Options: mobilenet_v3_small, efficientnet_b0, resnet50, resnet18\n",
    "    \"pretrained\": True,              # Use pretrained weights\n",
    "    \"batch_size\": 32,                # Batch size for training\n",
    "    \"image_size\": 224,               # Input image size\n",
    "    \"num_epochs\": 15,                # Number of training epochs\n",
    "    \"learning_rate\": 1e-4,           # Optimizer learning rate\n",
    "    \"optimizer\": \"adamw\",            # Options: adam, adamw, sgd\n",
    "    \"weight_decay\": 1e-3,            # Weight decay for AdamW\n",
    "    \"max_cpus\": 14,                  # Number of CPU threads for data loading\n",
    "    \"train_dir\": \"ms-coco/images/train-resized/train-resized\",  # Training images\n",
    "    \"label_dir\": \"ms-coco/labels/train/train\",                  # Training labels\n",
    "    \"test_dir\": \"ms-coco/images/test-resized/test-resized\",     # Test images\n",
    "    \"validation_split\": 0.2,         # Fraction of train set used for validation\n",
    "    \"threshold\": 0.5,                # Default threshold for multi-label predictions\n",
    "    \"freeze_backbone\": False,        # Freeze feature extractor/backbone for fine-tuning\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f8974",
   "metadata": {},
   "source": [
    "## Tools to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd4a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graphs(summary_writer, epoch, train_results, test_results,\n",
    "                  train_class_results=None, test_class_results=None, \n",
    "                  class_names = 80, mbatch_group=-1, mbatch_count=0, mbatch_losses=None):\n",
    "    if mbatch_group > 0:\n",
    "        for i in range(len(mbatch_losses)):\n",
    "            summary_writer.add_scalar(\"Losses/Train mini-batches\",\n",
    "                                  mbatch_losses[i],\n",
    "                                  epoch * mbatch_count + (i+1)*mbatch_group)\n",
    "\n",
    "    summary_writer.add_scalars(\"Losses/Train Loss vs Test Loss\",\n",
    "                               {\"Train Loss\" : train_results[\"loss\"],\n",
    "                                \"Test Loss\" : test_results[\"loss\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Accuracy vs Test Accuracy\",\n",
    "                               {\"Train Accuracy\" : train_results[\"accuracy\"],\n",
    "                                \"Test Accuracy\" : test_results[\"accuracy\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train F1 vs Test F1\",\n",
    "                               {\"Train F1\" : train_results[\"f1\"],\n",
    "                                \"Test F1\" : test_results[\"f1\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Precision vs Test Precision\",\n",
    "                               {\"Train Precision\" : train_results[\"precision\"],\n",
    "                                \"Test Precision\" : test_results[\"precision\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Recall vs Test Recall\",\n",
    "                               {\"Train Recall\" : train_results[\"recall\"],\n",
    "                                \"Test Recall\" : test_results[\"recall\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    if train_class_results and test_class_results:\n",
    "        for i in range(len(train_class_results)):\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train F1 vs Test F1\",\n",
    "                                       {\"Train F1\" : train_class_results[i][\"f1\"],\n",
    "                                        \"Test F1\" : test_class_results[i][\"f1\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train Precision vs Test Precision\",\n",
    "                                       {\"Train Precision\" : train_class_results[i][\"precision\"],\n",
    "                                        \"Test Precision\" : test_class_results[i][\"precision\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train Recall vs Test Recall\",\n",
    "                                       {\"Train Recall\" : train_class_results[i][\"recall\"],\n",
    "                                        \"Test Recall\" : test_class_results[i][\"recall\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "    summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db651a43",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e516bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_loop(train_loader, net, criterion, optimizer, device,\n",
    "               mbatch_loss_group=-1):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    mbatch_losses = []\n",
    "\n",
    "    for i, data in enumerate(tqdm(train_loader, desc=\"Training\", leave=True)):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # following condition False by default, unless mbatch_loss_group > 0\n",
    "        if i % mbatch_loss_group == mbatch_loss_group - 1:\n",
    "            mbatch_losses.append(running_loss / mbatch_loss_group)\n",
    "            running_loss = 0.0\n",
    "    if mbatch_loss_group > 0:\n",
    "        return mbatch_losses\n",
    "    else:\n",
    "        return running_loss / len(train_loader)  # add average loss return\n",
    "\n",
    "\n",
    "def validation_loop(val_loader, net, criterion, num_classes, device,\n",
    "                    multi_task=False, th_multi_task=0.5, one_hot=True, class_metrics=False):\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    size = len(val_loader.dataset)\n",
    "    class_total = {label:0 for label in range(num_classes)}\n",
    "    class_tp = {label:0 for label in range(num_classes)}\n",
    "    class_fp = {label:0 for label in range(num_classes)}\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(val_loader, desc=\"Validating\", leave=True):\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item() * images.size(0)\n",
    "            if not multi_task:    \n",
    "                predictions = torch.zeros_like(outputs)\n",
    "                predictions[torch.arange(outputs.shape[0]), torch.argmax(outputs, dim=1)] = 1.0\n",
    "            else:\n",
    "                predictions = torch.where(outputs > th_multi_task, 1.0, 0.0)\n",
    "            if not one_hot:\n",
    "                pass\n",
    "                \n",
    "            tps = predictions * labels\n",
    "            fps = predictions - tps\n",
    "            \n",
    "            tps = tps.sum(dim=0)\n",
    "            fps = fps.sum(dim=0)\n",
    "            lbls = labels.sum(dim=0)  \n",
    "                \n",
    "            for c in range(num_classes):\n",
    "                class_tp[c] += tps[c]\n",
    "                class_fp[c] += fps[c]\n",
    "                class_total[c] += lbls[c]\n",
    "                    \n",
    "            correct += tps.sum()\n",
    "\n",
    "    class_prec = []\n",
    "    class_recall = []\n",
    "    freqs = []\n",
    "    for c in range(num_classes):\n",
    "        class_prec.append(0 if class_tp[c] == 0 else\n",
    "                          class_tp[c] / (class_tp[c] + class_fp[c]))\n",
    "        class_recall.append(0 if class_tp[c] == 0 else\n",
    "                            class_tp[c] / class_total[c])\n",
    "        freqs.append(class_total[c])\n",
    "\n",
    "    freqs = torch.tensor(freqs)\n",
    "    class_weights = 1. / freqs\n",
    "    class_weights /= class_weights.sum()\n",
    "    class_prec = torch.tensor(class_prec)\n",
    "    class_recall = torch.tensor(class_recall)\n",
    "    prec = (class_prec * class_weights).sum()\n",
    "    recall = (class_recall * class_weights).sum()\n",
    "    f1 = 2. / (1/prec + 1/recall)\n",
    "    val_loss = loss / size\n",
    "    accuracy = correct / freqs.sum()\n",
    "    results = {\"loss\": val_loss, \"accuracy\": accuracy, \"f1\": f1,\\\n",
    "               \"precision\": prec, \"recall\": recall}\n",
    "\n",
    "    if class_metrics:\n",
    "        class_results = []\n",
    "        for p, r in zip(class_prec, class_recall):\n",
    "            f1 = (0 if p == r == 0 else 2. / (1/p + 1/r))\n",
    "            class_results.append({\"f1\": f1, \"precision\": p, \"recall\": r})\n",
    "        results = results, class_results\n",
    "\n",
    "    return results\n",
    "\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_val_probs_and_labels(val_loader, model, device):\n",
    "    \"\"\"Collect raw sigmoid probabilities and labels from the validation set.\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = model(images)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        all_probs.append(probs.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "    return torch.cat(all_probs, dim=0), torch.cat(all_labels, dim=0)\n",
    "\n",
    "\n",
    "def f1_weighted_precision_recall(preds, labels):\n",
    "    preds = preds.float()\n",
    "    labels = labels.float()\n",
    "\n",
    "    tps = (preds * labels).sum(dim=0)\n",
    "    fps = (preds * (1 - labels)).sum(dim=0)\n",
    "    freqs = labels.sum(dim=0).clamp(min=1e-9)\n",
    "\n",
    "    class_prec = tps / (tps + fps + 1e-9)\n",
    "    class_recall = tps / freqs\n",
    "\n",
    "    class_weights = 1.0 / freqs\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "    prec = (class_prec * class_weights).sum().item()\n",
    "    rec = (class_recall * class_weights).sum().item()\n",
    "    f1 = 0.0 if (prec == 0.0 and rec == 0.0) else 2.0 / (1.0 / prec + 1.0 / rec)\n",
    "    return f1, prec, rec\n",
    "\n",
    "\n",
    "def sweep_thresholds(probs, labels, thresholds=None, prefer_precision=None):\n",
    "    \"\"\"\n",
    "    Try several thresholds to find the one giving best F1.\n",
    "    If prefer_precision is set (e.g. 0.4), keep only thresholds with precision >= that.\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = [i / 100 for i in range(5, 96, 5)]  # 0.05 .. 0.95\n",
    "\n",
    "    results = []\n",
    "    for t in thresholds:\n",
    "        preds = (probs >= t).float()\n",
    "        f1, p, r = f1_weighted_precision_recall(preds, labels)\n",
    "        results.append((t, f1, p, r))\n",
    "\n",
    "    if prefer_precision is not None:\n",
    "        eligible = [x for x in results if x[2] >= prefer_precision]\n",
    "        if eligible:\n",
    "            return max(eligible, key=lambda x: x[1])  # best F1 among precision >= target\n",
    "\n",
    "    return max(results, key=lambda x: x[1])  # best F1 overall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56826c4d",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main training script for COCO multi-label classification.\n",
    "Supports optional freezing of backbone layers.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.models import (\n",
    "    mobilenet_v3_small, MobileNet_V3_Small_Weights,\n",
    "    resnet18, ResNet18_Weights,\n",
    ")\n",
    "\n",
    "# Local modules\n",
    "from dataset import COCOTrainImageDataset, COCOTestImageDataset\n",
    "from loops import train_loop, validation_loop\n",
    "from utils import (\n",
    "    update_graphs,\n",
    "    collect_val_probs_and_labels,\n",
    "    sweep_thresholds,\n",
    ")\n",
    "from config import CONFIG\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "\n",
    "def get_preprocessing_transform(model_name: str, train: bool = True, pretrained: bool = True) -> transforms.Compose:\n",
    "    \"\"\"Return preprocessing transforms depending on model and phase.\"\"\"\n",
    "    model_name = model_name.lower()\n",
    "\n",
    "    # Select normalization weights\n",
    "    if model_name == \"mobilenet_v3_small\":\n",
    "        weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    elif model_name == \"resnet18\":\n",
    "        weights = ResNet18_Weights.DEFAULT if pretrained else None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    if weights is not None:\n",
    "        norm_mean = weights.transforms().mean\n",
    "        norm_std = weights.transforms().std\n",
    "    else:\n",
    "        norm_mean = [0.485, 0.456, 0.406]\n",
    "        norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    if train:\n",
    "        # Training transform with augmentation\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(CONFIG[\"image_size\"], scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=norm_mean, std=norm_std),\n",
    "        ])\n",
    "\n",
    "    # Validation/test transform (deterministic)\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((CONFIG[\"image_size\"], CONFIG[\"image_size\"])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=norm_mean, std=norm_std),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# Model selection and modification\n",
    "\n",
    "def get_model(config: dict, device: torch.device, freeze_backbone: bool = False) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Load base model, optionally freeze backbone, and modify classifier for 80 COCO classes.\n",
    "    \"\"\"\n",
    "    model_name = config[\"model\"].lower()\n",
    "    pretrained = config.get(\"pretrained\", True)\n",
    "\n",
    "    if model_name == \"mobilenet_v3_small\":\n",
    "        weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        model = mobilenet_v3_small(weights=weights)\n",
    "        in_features = model.classifier[3].in_features\n",
    "        model.classifier[3] = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features, 80),\n",
    "        )\n",
    "        if freeze_backbone:\n",
    "            # Freeze all layers except classifier\n",
    "            for param in model.features.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    elif model_name == \"resnet18\":\n",
    "        weights = ResNet18_Weights.DEFAULT if pretrained else None\n",
    "        model = resnet18(weights=weights)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 80)\n",
    "        if freeze_backbone:\n",
    "            # Freeze all layers except final fully connected\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"fc\" not in name:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {config['model']}\")\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "# Training pipeline\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    # Device setup \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU detected. Using CUDA.\")\n",
    "    else:\n",
    "        print(\"No GPU detected. Training on CPU.\")\n",
    "        choice = input(\"Continue on CPU? (y/n): \").strip().lower()\n",
    "        if choice == \"y\":\n",
    "            device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            print(\"Exiting. Please use a GPU machine.\")\n",
    "            return\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Datasets and loaders \n",
    "    train_transform = get_preprocessing_transform(CONFIG[\"model\"], train=True)\n",
    "    val_transform = get_preprocessing_transform(CONFIG[\"model\"], train=False)\n",
    "\n",
    "    full_train_dataset = COCOTrainImageDataset(\n",
    "        img_dir=CONFIG[\"train_dir\"],\n",
    "        annotations_dir=CONFIG[\"label_dir\"],\n",
    "        transform=train_transform,\n",
    "    )\n",
    "    test_dataset = COCOTestImageDataset(\n",
    "        img_dir=CONFIG[\"test_dir\"],\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    val_size = int(CONFIG[\"validation_split\"] * len(full_train_dataset))\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=CONFIG[\"max_cpus\"])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"max_cpus\"])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"max_cpus\"])\n",
    "\n",
    "    print(f\"DataLoaders ready: train={len(train_loader)}, val={len(val_loader)}, test={len(test_loader)}\")\n",
    "\n",
    "    # Check sample batch\n",
    "    image, label = next(iter(train_loader))\n",
    "    print(f\"Sample batch shapes - images: {image.shape}, labels: {label.shape}\")\n",
    "\n",
    "    # Model, loss, optimizer \n",
    "    freeze_backbone = CONFIG.get(\"freeze_backbone\", False)\n",
    "    model = get_model(CONFIG, device, freeze_backbone=freeze_backbone)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer_name = CONFIG[\"optimizer\"].lower()\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=CONFIG[\"learning_rate\"], momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {CONFIG['optimizer']}\")\n",
    "\n",
    "    # TensorBoard logging \n",
    "    run_name = f\"{CONFIG['model']}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    writer = SummaryWriter(log_dir=os.path.join(\"runs\", run_name))\n",
    "    print(f\"TensorBoard logs at: runs/{run_name}\")\n",
    "\n",
    "    # Training loop \n",
    "    best_f1 = 0.0\n",
    "    best_model_path = \"best_model.pth\"\n",
    "\n",
    "    for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "\n",
    "        _ = train_loop(train_loader, model, criterion, optimizer, device)\n",
    "        train_results = validation_loop(train_loader, model, criterion, num_classes=80, device=device, multi_task=True)\n",
    "        val_results = validation_loop(val_loader, model, criterion, num_classes=80, device=device, multi_task=True)\n",
    "\n",
    "        update_graphs(writer, epoch, train_results, val_results)\n",
    "        # Each time we save the layer with the best F1 because \n",
    "        # the challenge is about to have the best F1\n",
    "        if val_results[\"f1\"] > best_f1:\n",
    "            best_f1 = val_results[\"f1\"]\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with F1 = {best_f1:.4f}\")\n",
    "\n",
    "    # Threshold sweep \n",
    "    val_probs, val_labels = collect_val_probs_and_labels(val_loader, model, device)\n",
    "    best_t, best_f1_sweep, best_prec, best_rec = sweep_thresholds(val_probs, val_labels)\n",
    "    print(f\"Best threshold = {best_t:.2f} | F1 = {best_f1_sweep:.4f} | Precision = {best_prec:.4f} | Recall = {best_rec:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from multiprocessing import freeze_support\n",
    "    freeze_support()\n",
    "    main()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5852c",
   "metadata": {},
   "source": [
    "# Test Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  test.py — Inference\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from dataset import COCOTestImageDataset   # <- make sure you use your correct dataset file\n",
    "from main import get_model                 # we reuse your get_model() to build the network\n",
    "from config import CONFIG\n",
    "\n",
    "from torchvision.models import (\n",
    "    mobilenet_v3_small, MobileNet_V3_Small_Weights,\n",
    "    efficientnet_b0, EfficientNet_B0_Weights,\n",
    "    resnet50, ResNet50_Weights\n",
    ")\n",
    "\n",
    "\n",
    "def get_test_transform(config):\n",
    "    \"\"\"\n",
    "    Returns the deterministic test transform for the selected model.\n",
    "    Uses the same preprocessing as the original pretrained weights.\n",
    "    \"\"\"\n",
    "    model_name = config[\"model\"].lower()\n",
    "    pretrained = config.get(\"pretrained\", True)\n",
    "\n",
    "    if model_name == \"mobilenet_v3_small\":\n",
    "        weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    elif model_name == \"efficientnet_b0\":\n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    elif model_name == \"resnet50\":\n",
    "        weights = ResNet50_Weights.DEFAULT if pretrained else None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {config['model']}\")\n",
    "\n",
    "    if weights is not None:\n",
    "        return weights.transforms()\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Device \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU detected. Using CUDA for inference.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"No GPU detected. Using CPU.\")\n",
    "\n",
    "    # Dataset & DataLoader \n",
    "    test_transform = get_test_transform(CONFIG)\n",
    "    test_dataset = COCOTestImageDataset(\n",
    "        img_dir=CONFIG[\"test_dir\"],\n",
    "        transform=test_transform\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG[\"max_cpus\"]\n",
    "    )\n",
    "\n",
    "    # Load model \n",
    "    model = get_model(CONFIG, device)\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Loaded model from best_model.pth — starting inference on {len(test_dataset)} images...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prediction loop\n",
    "    predictions = {}\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs >= CONFIG[\"threshold\"]).cpu().numpy()\n",
    "\n",
    "            for img_id, pred in zip(image_ids, preds):\n",
    "                predictions[img_id] = pred.nonzero()[0].tolist()\n",
    "\n",
    "    # Save results \n",
    "    output_json = \"test_predictions.json\"\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(predictions, f, indent=4)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nInference complete! Processed {len(test_dataset)} images \"\n",
    "          f\"in {elapsed/60:.2f} min ({elapsed:.1f} s total).\")\n",
    "    print(f\"Predictions saved to {output_json}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from multiprocessing import freeze_support\n",
    "    freeze_support()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4855a7",
   "metadata": {},
   "source": [
    "## 5. The skeleton of the test submission program\n",
    "\n",
    "This, much simpler, program should have the following sections and control flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c713a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COCOTestImageDataset   \u001b[38;5;66;03m# <- make sure you use your correct dataset file\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model                 \u001b[38;5;66;03m# we reuse your get_model() to build the network\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CONFIG\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     mobilenet_v3_small, MobileNet_V3_Small_Weights,\n\u001b[1;32m     18\u001b[0m     efficientnet_b0, EfficientNet_B0_Weights,\n\u001b[1;32m     19\u001b[0m     resnet50, ResNet50_Weights\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-INSALyon/TC-5A-2025-2026/IAT/ProjetIAV/main.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, random_split\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     mobilenet_v3_small, MobileNet_V3_Small_Weights,\n\u001b[1;32m     16\u001b[0m     resnet18, ResNet18_Weights,\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/torch/utils/tensorboard/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_vendor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensorboard, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m Version(\n\u001b[1;32m      5\u001b[0m     tensorboard\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m      6\u001b[0m ) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.15\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "#  test.py — Inference\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from dataset import COCOTestImageDataset   # <- make sure you use your correct dataset file\n",
    "from main import get_model                 # we reuse your get_model() to build the network\n",
    "from config import CONFIG\n",
    "\n",
    "from torchvision.models import (\n",
    "    mobilenet_v3_small, MobileNet_V3_Small_Weights,\n",
    "    efficientnet_b0, EfficientNet_B0_Weights,\n",
    "    resnet50, ResNet50_Weights\n",
    ")\n",
    "\n",
    "\n",
    "def get_test_transform(config):\n",
    "    \"\"\"\n",
    "    Returns the deterministic test transform for the selected model.\n",
    "    Uses the same preprocessing as the original pretrained weights.\n",
    "    \"\"\"\n",
    "    model_name = config[\"model\"].lower()\n",
    "    pretrained = config.get(\"pretrained\", True)\n",
    "\n",
    "    if model_name == \"mobilenet_v3_small\":\n",
    "        weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    elif model_name == \"efficientnet_b0\":\n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    elif model_name == \"resnet50\":\n",
    "        weights = ResNet50_Weights.DEFAULT if pretrained else None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {config['model']}\")\n",
    "\n",
    "    if weights is not None:\n",
    "        return weights.transforms()\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU detected. Using CUDA for inference.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"No GPU detected. Using CPU.\")\n",
    "\n",
    "    # Dataset & DataLoader \n",
    "    test_transform = get_test_transform(CONFIG)\n",
    "    test_dataset = COCOTestImageDataset(\n",
    "        img_dir=CONFIG[\"test_dir\"],\n",
    "        transform=test_transform\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG[\"max_cpus\"]\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    model = get_model(CONFIG, device)\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Loaded model from best_model.pth — starting inference on {len(test_dataset)} images...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prediction loop \n",
    "    predictions = {}\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs >= CONFIG[\"threshold\"]).cpu().numpy()\n",
    "\n",
    "            for img_id, pred in zip(image_ids, preds):\n",
    "                predictions[img_id] = pred.nonzero()[0].tolist()\n",
    "\n",
    "    # Save results \n",
    "    output_json = \"test_predictions.json\"\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(predictions, f, indent=4)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nInference complete! Processed {len(test_dataset)} images \"\n",
    "          f\"in {elapsed/60:.2f} min ({elapsed:.1f} s total).\")\n",
    "    print(f\"Predictions saved to {output_json}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from multiprocessing import freeze_support\n",
    "    freeze_support()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8106c",
   "metadata": {},
   "source": [
    "## 6. Our Approach\n",
    "\n",
    "The purpose of the task was, to our eyes, to find the best \"good enough\" model and parameters settings to answer to te question of multi-label classification for this specific dataset. We had the freedom to chose from a large list of pre-trained models, available in the `PyTorch` library, to help us with our task. \n",
    "\n",
    "This task alone is very time consuming: it requires testing the classification by modifying a couple of parameters, all while also testing between different models to see which one is the best fitted. We understood we were gonna have to make some decisions if we wanted to optimize our work. Our approach was then the following:\n",
    "\n",
    "- choosing a couple of models from the PyTorch list \n",
    "- choosing a couple of parameters that we would modify along the testing phase\n",
    "\n",
    "It was very important for us to set these two steps, if not, the work would have been too long to produce. Also, chosing the \"best\" model is impossible for us, due to an obvious lack of ressources and time.\n",
    "\n",
    "We eventually decided to choose the models based on an energy focus. We know how much energy and resource consuming is the training of AI models nowadays. This pressing concern is largely discussed and raises questions about the ethical use of these tools. It made us think of trying to find the best working model out of the most reduced ones, that is, those with the less parameters, all for a purpose of saving time and ressources. We understood that by taking this decision, our final model would perform worse once updated to the leaderboard comapared to other, bigger models. We still decided to search between the couple rather small models in the `PyTorch` distribution, trying to come up with the one that is \"good enough\". The models we decided to test are the following:\n",
    "\n",
    "- MobileNetV3-Small\n",
    "- ResNet-18\n",
    "\n",
    "We deliberately focused on these two architectures because they are not only computationally efficient but also well suited to multi-label image classification from an architectural standpoint. Also because we studied them in class. They have a small amount of parameters compared to other architectures. \n",
    "\n",
    "MobileNetV3-Small was designed specifically for low-resource environments. Architecturally, it combines depthwise separable convolutions (greatly reducing the number of parameters and multiplications) with inverted residual blocks and linear bottlenecks, which mantain representational power while keeping the model extremely light. It also integrates squeeze-and-excitation attention blocks, improving the network’s ability to focus on informative channels, something important in multi-label problems where several objects may share the image. Also, its small parameter count makes it fast to train and low on memory and energy cost. It has 3 million parameters.\n",
    "\n",
    "On the other hand, ResNet-18 is a more classical but still efficient architecture that we studied in class. Its residual skip connections allow for deeper networks to train without vanishing gradients while still keeping the parameter count moderate compared to very deep ResNets and other models. Its parameter count is in the 11 million order. Although it uses standard convolutions (no depthwise separable trick), its straightforward structure is robust and proven for general image recognition. For a multi-label task, the residual connections help the model learn richer representations without becoming prohibitively heavy.\n",
    "\n",
    "We explicitly avoided architectures such as ConvNeXt, which, while state-of-the-art, are much deeper and more computationally expensive, and models like VGG or DenseNet, which are parameter-heavy with less efficient use of computation. Our two chosen models represent different CNN design evolutions: classic residual learning (ResNet-18) and mobile-optimized depthwise convolutions (MobileNetV3). This mix gives us a practical space to experiment with accuracy vs efficiency trade-offs for our dataset while keeping training feasible and energy-aware.\n",
    "\n",
    "We then chose which parameters we would focus on, before diving into the code and then the testing. There are a large number of parameters to experiment with when trying to perfectionate a model. We decided, again for a question of efficiency, to set a couple of them (the most impactful) so we could make an honest study, avoiding changing of parameters between models. These are the parameters, which we divided into two sections:\n",
    "\n",
    "- efficiency parameters:\n",
    "    - number of cpus\n",
    "    - batch size\n",
    "\n",
    "- learning parameters:\n",
    "    - number of epochs\n",
    "    - learning rate\n",
    "    - image size\n",
    "    - optimizer\n",
    "    - weight decay\n",
    "    - dropout layer\n",
    "    - threshold\n",
    "\n",
    "These are the most important parameters in terms of impact in a model's behaviour. There is a larger number of them, but we don't have the time to make a big testing pool with all of them.\n",
    "\n",
    "We then drew the lines for what would be our attack. We first set up our working environment, that is, setting up the `Github` page for our code and a shared `Drive` for our results (excel and word). We then needed to make a `main.py` code for the training of our models, and a `test.py` for the generation of the JSON file we would eventually upload to the leaderboard. The main modules of the code were already given in this notebook. Then, each choosing one model, we would test its perfomance while modifying its parameters to find the best working one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b109d61",
   "metadata": {},
   "source": [
    "## 8. Results and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952d71a",
   "metadata": {},
   "source": [
    "## 8.1 MobileNetV3-Small\n",
    "\n",
    "We trained **MobileNetV3-Small** without freezing layers to maintain full precision. The goal was to balance **training speed**, **generalization**, and **F1 performance**.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.1.1 Initial Runs (Run 1–3)\n",
    "\n",
    "- **Run 1:** Quick test to estimate training time and initial F1 score. We also wanted to check which were gonna be the ideal efficiency parameters for our model.\n",
    "  - **Parameters:** batch size 32, epochs 5, lr 0.001, image size 224, optimizer Adam, no dropout, threshold 0.5  \n",
    "  - **Best F1:** 0.3585, **Time:** 17 min  \n",
    "\n",
    "  ![Run 1](images/1.png)\n",
    "\n",
    "- **Run 2:** Increased epochs to 8; F1 remained similar.  \n",
    "  - **Parameters:** batch size 32, epochs 8, lr 0.001, image size 224, optimizer Adam, no dropout, threshold 0.5  \n",
    "  - **Best F1:** 0.3555, **Time:** 20 min  \n",
    "\n",
    "- **Run 3:** Increased batch size to 64 and epochs to 15; slight F1 improvement but signs of overfitting appeared.  \n",
    "  - **Parameters:** batch size 64, epochs 15, lr 0.001, image size 224, optimizer Adam, no dropout, threshold 0.5  \n",
    "  - **Best F1:** 0.3786, **Time:** 33 min  \n",
    "\n",
    "---\n",
    "\n",
    "### 8.1.2 Generalization Improvements (Run 4–5)\n",
    "\n",
    "- **Run 4:** Changed optimizer to `AdamW`, reduced learning rate, increased image size.  \n",
    "  - **Parameters:** batch size 64, epochs 10, lr 0.0001, image size 256, optimizer AdamW, weight decay 0.0001, no dropout, threshold 0.5  \n",
    "  - **Best F1:** 0.3814, **Time:** 42 min  \n",
    "\n",
    "  ![Run 4](images/5.png)  \n",
    "  ![Run 4 Metrics](images/6.png)  \n",
    "  ![Run 4 Metrics](images/7.png)\n",
    "\n",
    "- **Run 5:** Added dropout and increased weight decay; final F1 similar to run 4.  \n",
    "  - **Parameters:** batch size 64, epochs 10, lr 0.0003, image size 256, optimizer AdamW, weight decay 0.0003, dropout Yes, threshold 0.5  \n",
    "  - **Best F1:** 0.38, **Time:** 45 min  \n",
    "\n",
    "---\n",
    "\n",
    "### 8.1.3 Aggressive Generalization & Threshold Tuning (Run 6–7)\n",
    "\n",
    "- **Run 6:** Increased epochs to 20, added `ColorJitter`, higher weight decay. Validation F1 improved but test F1 dropped; training time too long.  \n",
    "  - **Parameters:** batch size 64, epochs 20, lr 0.0003, image size 256, optimizer AdamW, weight decay 0.001, dropout Yes, threshold 0.5  \n",
    "\n",
    "- **Run 7:** Reduced epochs, batch size, and image size; applied **threshold sweeping** to optimize precision/recall balance.  \n",
    "  - **Parameters:** batch size 32, epochs 15, lr 0.0001, image size 224, optimizer AdamW, weight decay 0.001, dropout Yes, threshold Sweeping  \n",
    "  - **Best F1:** 0.3971, **Time:** 69 min  \n",
    "\n",
    "  ![Run 7 Metrics](images/8.png)  \n",
    "  ![Run 7 Metrics](images/9.png)\n",
    "\n",
    "The gaps between the precision in the validation set and in the training set diminished after this run, so we decided it was our best \"good enough\" configuration for this precise model, all while trying to keep it small.\n",
    "\n",
    "We had managed to reduce the overfitting problem that our model was having.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.1.4 Summary Table\n",
    "\n",
    "| Run | Batch Size | Epochs | LR      | Image Size | Optimizer | Weight Decay | Dropout | Threshold | Best F1 | Time (min) |\n",
    "|-----|-----------|--------|---------|------------|-----------|--------------|---------|-----------|---------|------------|\n",
    "| 1   | 32        | 5      | 0.001   | 224        | Adam      | 0            | No      | 0.5       | 0.3585  | 17         |\n",
    "| 2   | 32        | 8      | 0.001   | 224        | Adam      | 0            | No      | 0.5       | 0.3555  | 20         |\n",
    "| 3   | 64        | 15     | 0.001   | 224        | Adam      | 0            | No      | 0.5       | 0.3786  | 33         |\n",
    "| 4   | 64        | 10     | 0.0001  | 256        | AdamW     | 0.0001       | No      | 0.5       | 0.3814  | 42         |\n",
    "| 5   | 64        | 10     | 0.0003  | 256        | AdamW     | 0.0003       | Yes     | 0.5       | 0.38    | 45         |\n",
    "| 6   | 64        | 20     | 0.0003  | 256        | AdamW     | 0.001        | Yes     | 0.5       | -       | -          |\n",
    "| 7   | 32        | 15     | 0.0001  | 224        | AdamW     | 0.001        | Yes     | Sweeping  | 0.3971  | 69         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e282bc3",
   "metadata": {},
   "source": [
    "## 8.2 Resnet 18\n",
    "\n",
    "### 8.2.1 Introduction\n",
    "\n",
    "ResNet-18 is a deep learning model with 18 layers and about 11.7M weights. It uses skip connections to train more effectively.\n",
    "\n",
    "We use it with the MS COCO dataset because it’s lightweight, fast, and still powerful enough to learn useful features from COCO’s large variety of images and objects.\n",
    "\n",
    "To train this model we changed computer to also test training with a faster computer. We use an an NVIDIA RTX A2000 8GB and I7 CPU.\n",
    "\n",
    "### 8.2.2 Why we chose ResNet-18\n",
    "\n",
    "\n",
    "Efficiency: ResNet-18 is computationally less demanding compared to deeper models, making it suitable for tasks requiring faster inference times. That means less Time to wait and less power consumtion it's a main value for us.\n",
    "\n",
    "Transfer Learning: Pretrained ResNet-18 models on ImageNet can be fine-tuned for COCO, leveraging learned features for improved performance.\n",
    "\n",
    "Proven Architecture: ResNet architectures, including ResNet-18, have demonstrated strong performance in various vision tasks, including classification.\n",
    "\n",
    "\n",
    "\n",
    "### 8.2.3 Let's Run\n",
    "\n",
    "--\n",
    "\n",
    "#### A. Initial Run with ResNet18 (Unfrozen)\n",
    "\n",
    "For our first experiment, we used the same parameters as MobileNet to establish a baseline and identify potential areas for improvement.\n",
    "\n",
    "- **Setup:** ResNet18 with all layers trainable  \n",
    "- **Training:** 2 hours, 10 epochs  \n",
    "- **Learning rate:** 1e-4  \n",
    "\n",
    "**Results:**  \n",
    "- F1 score: 0.42  \n",
    "- Loss: still decreasing  \n",
    "\n",
    "**Observations:**  \n",
    "- The model was still learning, suggesting that increasing the number of epochs could improve results.  \n",
    "- Increasing the learning rate to 1e-3 might allow faster convergence.  \n",
    "- Early visualizations indicated that certain classes were underrepresented, hinting at a potential data imbalance issue.\n",
    "\n",
    "![Alt text](images/Metrics-Resnet18-1.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Freezing the Backbone\n",
    "\n",
    "Next, we froze the backbone of ResNet18 to focus training on the classifier layers.\n",
    "\n",
    "- **Setup:** Backbone frozen, fine-tuning only the classifier  \n",
    "- **Training:** 11 epochs  \n",
    "\n",
    "**Results:**  \n",
    "- F1 score after 11 epochs: 0.51  \n",
    "- F1 score after 10 epochs: 0.46  \n",
    "\n",
    "**Observations:**  \n",
    "- Overlearning observed at 11 epochs, with accuracy decreasing.  \n",
    "- Adjustments made to address Overlearning:\n",
    "  - Reduce epochs to 10  \n",
    "  - Add a dropout layer with p = 0.4  \n",
    "  - Resize images to 224 × 224 (ResNet18’s original size for better performance)  \n",
    "\n",
    "**Note:**  \n",
    "- Dropout run gave an F1 score of 0.010 after 3 epochs, so it was stopped early.  \n",
    "- Suggests that dropout placement or value might need tuning rather than a blanket addition.\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Freezing Backbone + Resetting Last Layer\n",
    "\n",
    "We experimented with freezing the backbone while resetting only the final layer weights.\n",
    "\n",
    "- **Setup:** Frozen backbone, last layer reinitialized  \n",
    "- **Training:** 10 epochs  \n",
    "- **Learning rate:** 1e-3  \n",
    "\n",
    "**Results:**  \n",
    "- F1 score: 0.48  \n",
    "- Training time reduced by ~40% compared to fully unfrozen model  \n",
    "\n",
    "**Observations:**  \n",
    "- Faster convergence due to fewer trainable parameters.  \n",
    "- Slightly lower F1 than full fine-tuning, indicating that some deeper layers might still benefit from fine-tuning.  \n",
    "- Suggested future approach: selectively unfreeze top blocks of the backbone.\n",
    "\n",
    "---\n",
    "\n",
    "#### D. Image Augmentation & Resizing\n",
    "\n",
    "To improve generalization, we applied augmentation strategies:\n",
    "\n",
    "- **Techniques:** Random horizontal flip, random rotation, color jitter  \n",
    "- **Image size:** 224 × 224  \n",
    "\n",
    "**Results:**  \n",
    "- F1 score improved slightly (~0.50)  \n",
    "- Reduced overlearning, especially on minority classes  \n",
    "\n",
    "**Observations:**  \n",
    "- Augmentation helped balance class performance.  \n",
    "- Further improvement could be achieved by adding advanced augmentation like CutMix or MixUp.\n",
    "\n",
    "---\n",
    "\n",
    "#### E. Learning Rate Tuning\n",
    "\n",
    "We experimented with a higher learning rate and learning rate scheduling:\n",
    "\n",
    "- **Setup:** Initial LR = 1e-3, ReduceLROnPlateau scheduler  \n",
    "- **Training:** 10 epochs  \n",
    "\n",
    "**Results:**  \n",
    "- F1 score increased to 0.52\n",
    "- Loss plateaued earlier but stabilized  \n",
    "\n",
    "**Observations:**  \n",
    "- Adaptive LR helped avoid overshooting minima.  \n",
    "- Future runs could combine scheduler with fine-tuned backbone layers.\n",
    "\n",
    "---\n",
    "\n",
    "#### F. Unfreezing Backbone Mid-Training\n",
    "\n",
    "We tested the effect of unfreezing the backbone after 10 epochs of frozen training, then continuing to 20 epochs.\n",
    "\n",
    "- **Setup:** Backbone frozen for first 10 epochs, then unfrozen  \n",
    "- **Training:** 20 epochs total  \n",
    "- **Learning rate:** 1e-3  \n",
    "\n",
    "**Results:**  \n",
    "- F1 score: 0.49  \n",
    "\n",
    "**Observations:**  \n",
    "- Unfreezing the backbone mid-training improved performance over frozen-only runs but did not surpass fully tuned methods.  \n",
    "- Suggests that gradual unfreezing can help, but careful learning rate adjustment may be needed when unfreezing. We didn't do it.\n",
    "- We can see that wen we remove the freeze we got big change in the curbes and high learning rates.\n",
    "\n",
    "![Alt text](images/RunChangingFreeze.png)\n",
    "![Alt text](images/RunChangingFreeze_2.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 8.2.4 Summary of Runs\n",
    "\n",
    "| Run | Backbone | Dropout | Augmentation | Epochs | Learning Rate    | F1 Score | Notes |\n",
    "|-----|----------|---------|--------------|--------|-------|----------|-------|\n",
    "| 1   | Unfrozen | None    | None         | 10     | 1e-4  | 0.42     | Baseline, loss still decreasing |\n",
    "| 2   | Frozen   | None    | None         | 11     | 1e-4  | 0.51     | Overlearning observed |\n",
    "| 2a  | Frozen   | 0.4     | None         | 3      | 1e-4  | 0.000001     | Dropout too strong, stopped early |\n",
    "| 3   | Frozen   | None    | None         | 10     | 1e-3  | 0.48     | Last layer reset, faster training |\n",
    "| 4   | Frozen   | None    | Augmentation | 10     | 1e-3  | 0.50     | Better generalization |\n",
    "| 5   | Frozen   | None    | Augmentation | 10     | 1e-3 (sched) | 0.52 | Best F1, adaptive LR helped |\n",
    "| 6   | Frozen→Unfrozen | None | Augmentation | 20 | 1e-3 | 0.49 | Unfroze backbone mid-training |\n",
    "\n",
    "---\n",
    "\n",
    "### 8.2.5 Conclusion\n",
    "\n",
    "We observed that regardless of changing the parameters or the approach, we obtained approximately the same F1 score and loss. We don’t believe we can improve much with this model, as it is a lightweight one. We should be able to reach around 0.60, but not much higher. The images from all the runs show that the results remain roughly the same.\n",
    "\n",
    "![Alt text](images/Allruns.png)\n",
    "\n",
    "**Next Steps:**  \n",
    "- Experiment with unfreezing top blocks of backbone earlier.  \n",
    "- Fine-tune dropout placement and probability.  \n",
    "- Test advanced augmentation methods.  \n",
    "- Explore class-balanced loss or oversampling for minority classes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d96198",
   "metadata": {},
   "source": [
    "## 9. To go further\n",
    "\n",
    "We decided as a future case of study to try other models results on this task. We tried ConvNext.\n",
    "\n",
    "ConvNeXt is a modern reinterpretation of the classic ResNet architecture, redesigned with ideas borrowed from Vision Transformers while keeping a fully convolutional backbone. It simplifies the traditional residual blocks into depthwise separable convolutions with large kernels (7×7), layer normalization instead of batch normalization, and inverted bottlenecks similar to those in efficient mobile models. This design improves the receptive field and feature extraction capacity. It is therefore a big model with a large number of parameters and a large computional time, which derives from our main approach to the task. \n",
    "\n",
    "We just wanted to compare our results with those of other models. We found out the F1 was better when working with a model like this, but the computation time and energy gone into it derived from our objective. It could still be a nice track of studying for the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
