{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98da5ac3",
   "metadata": {},
   "source": [
    "# The MS COCO classification challenge\n",
    "\n",
    "Razmig Kéchichian\n",
    "\n",
    "This notebook defines the multi-class classification challenge on the [MS COCO dataset](https://cocodataset.org/). It defines the problem, sets the rules of organization and presents tools you are provided with to accomplish the challenge.\n",
    "\n",
    "\n",
    "## 1. Problem statement\n",
    "\n",
    "Each image has **several** categories of objects to predict, hence the difference compared to the classification problem we have seen on the CIFAR10 dataset where each image belonged to a **single** category, therefore the network loss function and prediction mechanism (only highest output probability) were defined taking this constraint into account.\n",
    "\n",
    "We adapted the MS COCO dataset for the requirements of this challenge by, among other things, reducing the number of images and their dimensions to facilitate processing.\n",
    "\n",
    "In the companion `ms-coco.zip` compressed directory you will find two sub-directories:\n",
    "- `images`: which contains the images in train (65k) and test (~5k) subsets,\n",
    "- `labels`: which lists labels for each of the images in the train subset only.\n",
    "\n",
    "Each label file gives a list of class IDs that correspond to the class index in the following tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \n",
    "           \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
    "           \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",       \n",
    "           \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "           \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "           \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \n",
    "           \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \n",
    "           \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \n",
    "           \"hair drier\", \"toothbrush\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf52f93",
   "metadata": {},
   "source": [
    "Your goal is to follow a **transfer learning strategy** in training and validating a network on **your own distribution of training data into training and a validation subsets**, then to **test it on the test subset** by producing a [JSON file](https://en.wikipedia.org/wiki/JSON) with content of the following format:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"000000000139\": [\n",
    "        56,\n",
    "        60,\n",
    "        62\n",
    "    ],\n",
    "    \"000000000285\": [\n",
    "        21,\n",
    "    ],\n",
    "    \"000000000632\": [\n",
    "        57,\n",
    "        59,\n",
    "    73\n",
    "    ],\n",
    "    # other test images\n",
    "}\n",
    "```\n",
    "\n",
    "In this file, the name (without extension) of each test image is associated with a list of class indices predicted by your network. Make sure that the JSON file you produce **follows this format strictly**.\n",
    "\n",
    "You will submit your JSON prediction file to the following [online evaluation server and leaderboard](https://www.creatis.insa-lyon.fr/kechichian/ms-coco-classif-leaderboard.html), which will evaluate your predictions on test set labels, unavailable to you.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>WARNING:</b> Use this server with <b>the greatest care</b>. A new submission with identical Participant or group name will <b>overwrite</b> the identically named submission, if one already exists, therefore check the leaderboard first. <b>Do not make duplicate leaderboard entries for your group</b>, keep track of your test scores privately. Also pay attention to upload only JSON files of the required format.<br>\n",
    "</div>\n",
    "\n",
    "The evaluation server calculates and returns mean performances over all classes, and optionally per class performances. Entries in the leaderboard are sorted by the F1 metric.\n",
    "\n",
    "You can request an evaluation as many times as you want. It is up to you to specify the final evaluation by updating the leaderboard entry corresponding to your Participant or group name. This entry will be taken into account for grading your work.\n",
    "\n",
    "It goes without saying that it is **prohibited** to use another distribution of the MS COCO database for training, e.g. the Torchvision dataset.\n",
    "\n",
    "\n",
    "## 2. Organization\n",
    "\n",
    "- Given the scope of the project, you will work in groups of 2. \n",
    "- Work on the challenge begins on IAV lab 3 session, that is on the **23rd of September**.\n",
    "- Results are due 10 days later, that is on the **3rd of October, 18:00**. They comrpise:\n",
    "    - a submission to the leaderboard,\n",
    "    - a commented Python script (with any necessary modules) or Jupyter Notebook, uploaded on Moodle in the challenge repository by one of the members of the group.\n",
    "    \n",
    "    \n",
    "## 3. Tools\n",
    "\n",
    "In addition to the MS COCO annotated data and the evaluation server, we provide you with most code building blocks. Your task is to understand them and use them to create the glue logic, that is the main program, putting all these blocks together and completing them as necessary to implement a complete machine learning workflow to train and validate a model, and produce the test JSON file.\n",
    "\n",
    "### 3.1 Custom `Dataset`s\n",
    "\n",
    "We provide you with two custom `torch.utils.data.Dataset` sub-classes to use in training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class COCOTrainImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, annotations_dir, max_images=None, transform=None):\n",
    "        self.img_labels = sorted(glob(\"*.cls\", root_dir=annotations_dir))\n",
    "        if max_images:\n",
    "            self.img_labels = self.img_labels[:max_images]\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, Path(self.img_labels[idx]).stem + \".jpg\")\n",
    "        labels_path = os.path.join(self.annotations_dir, self.img_labels[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        with open(labels_path) as f: \n",
    "            labels = [int(label) for label in f.readlines()]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        labels = torch.zeros(80).scatter_(0, torch.tensor(labels), value=1)\n",
    "        return image, labels\n",
    "\n",
    "\n",
    "class COCOTestImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_list = sorted(glob(\"*.jpg\", root_dir=img_dir))    \n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_list[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, Path(img_path).stem # filename w/o extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d805e2",
   "metadata": {},
   "source": [
    "### 3.2 Training and validation loops\n",
    "\n",
    "The following are two general-purpose classification train and validation loop functions to be called inside the epochs for-loop with appropriate argument settings.\n",
    "\n",
    "Pay particular attention to the `validation_loop()` function's arguments `multi_task`, `th_multi_task` and `one_hot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb693462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def train_loop(train_loader, net, criterion, optimizer, device,\n",
    "               mbatch_loss_group=-1):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    mbatch_losses = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # following condition False by default, unless mbatch_loss_group > 0\n",
    "        if i % mbatch_loss_group == mbatch_loss_group - 1:\n",
    "            mbatch_losses.append(running_loss / mbatch_loss_group)\n",
    "            running_loss = 0.0\n",
    "    if mbatch_loss_group > 0:\n",
    "        return mbatch_losses\n",
    "\n",
    "\n",
    "def validation_loop(val_loader, net, criterion, num_classes, device,\n",
    "                    multi_task=False, th_multi_task=0.5, one_hot=False, class_metrics=False):\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    size = len(val_loader.dataset)\n",
    "    class_total = {label:0 for label in range(num_classes)}\n",
    "    class_tp = {label:0 for label in range(num_classes)}\n",
    "    class_fp = {label:0 for label in range(num_classes)}\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item() * images.size(0)\n",
    "            if not multi_task:    \n",
    "                predictions = torch.zeros_like(outputs)\n",
    "                predictions[torch.arange(outputs.shape[0]), torch.argmax(outputs, dim=1)] = 1.0\n",
    "            else:\n",
    "                predictions = torch.where(outputs > th_multi_task, 1.0, 0.0)\n",
    "            if not one_hot:\n",
    "                labels_mat = torch.zeros_like(outputs)\n",
    "                labels_mat[torch.arange(outputs.shape[0]), labels] = 1.0\n",
    "                labels = labels_mat\n",
    "                \n",
    "            tps = predictions * labels\n",
    "            fps = predictions - tps\n",
    "            \n",
    "            tps = tps.sum(dim=0)\n",
    "            fps = fps.sum(dim=0)\n",
    "            lbls = labels.sum(dim=0)  \n",
    "                \n",
    "            for c in range(num_classes):\n",
    "                class_tp[c] += tps[c]\n",
    "                class_fp[c] += fps[c]\n",
    "                class_total[c] += lbls[c]\n",
    "                    \n",
    "            correct += tps.sum()\n",
    "\n",
    "    class_prec = []\n",
    "    class_recall = []\n",
    "    freqs = []\n",
    "    for c in range(num_classes):\n",
    "        class_prec.append(0 if class_tp[c] == 0 else\n",
    "                          class_tp[c] / (class_tp[c] + class_fp[c]))\n",
    "        class_recall.append(0 if class_tp[c] == 0 else\n",
    "                            class_tp[c] / class_total[c])\n",
    "        freqs.append(class_total[c])\n",
    "\n",
    "    freqs = torch.tensor(freqs)\n",
    "    class_weights = 1. / freqs\n",
    "    class_weights /= class_weights.sum()\n",
    "    class_prec = torch.tensor(class_prec)\n",
    "    class_recall = torch.tensor(class_recall)\n",
    "    prec = (class_prec * class_weights).sum()\n",
    "    recall = (class_recall * class_weights).sum()\n",
    "    f1 = 2. / (1/prec + 1/recall)\n",
    "    val_loss = loss / size\n",
    "    accuracy = correct / freqs.sum()\n",
    "    results = {\"loss\": val_loss, \"accuracy\": accuracy, \"f1\": f1,\\\n",
    "               \"precision\": prec, \"recall\": recall}\n",
    "\n",
    "    if class_metrics:\n",
    "        class_results = []\n",
    "        for p, r in zip(class_prec, class_recall):\n",
    "            f1 = (0 if p == r == 0 else 2. / (1/p + 1/r))\n",
    "            class_results.append({\"f1\": f1, \"precision\": p, \"recall\": r})\n",
    "        results = results, class_results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d8e84",
   "metadata": {},
   "source": [
    "### 3.3 Tensorboard logging (optional)\n",
    "\n",
    "Evaluation metrics and losses produced by the `validation_loop()` function on train and validation data can be logged to a [Tensorboard `SummaryWriter`](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) which allows you to observe training graphically via the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graphs(summary_writer, epoch, train_results, test_results,\n",
    "                  train_class_results=None, test_class_results=None, \n",
    "                  class_names = None, mbatch_group=-1, mbatch_count=0, mbatch_losses=None):\n",
    "    if mbatch_group > 0:\n",
    "        for i in range(len(mbatch_losses)):\n",
    "            summary_writer.add_scalar(\"Losses/Train mini-batches\",\n",
    "                                  mbatch_losses[i],\n",
    "                                  epoch * mbatch_count + (i+1)*mbatch_group)\n",
    "\n",
    "    summary_writer.add_scalars(\"Losses/Train Loss vs Test Loss\",\n",
    "                               {\"Train Loss\" : train_results[\"loss\"],\n",
    "                                \"Test Loss\" : test_results[\"loss\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Accuracy vs Test Accuracy\",\n",
    "                               {\"Train Accuracy\" : train_results[\"accuracy\"],\n",
    "                                \"Test Accuracy\" : test_results[\"accuracy\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train F1 vs Test F1\",\n",
    "                               {\"Train F1\" : train_results[\"f1\"],\n",
    "                                \"Test F1\" : test_results[\"f1\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Precision vs Test Precision\",\n",
    "                               {\"Train Precision\" : train_results[\"precision\"],\n",
    "                                \"Test Precision\" : test_results[\"precision\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Recall vs Test Recall\",\n",
    "                               {\"Train Recall\" : train_results[\"recall\"],\n",
    "                                \"Test Recall\" : test_results[\"recall\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    if train_class_results and test_class_results:\n",
    "        for i in range(len(train_class_results)):\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train F1 vs Test F1\",\n",
    "                                       {\"Train F1\" : train_class_results[i][\"f1\"],\n",
    "                                        \"Test F1\" : test_class_results[i][\"f1\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train Precision vs Test Precision\",\n",
    "                                       {\"Train Precision\" : train_class_results[i][\"precision\"],\n",
    "                                        \"Test Precision\" : test_class_results[i][\"precision\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train Recall vs Test Recall\",\n",
    "                                       {\"Train Recall\" : train_class_results[i][\"recall\"],\n",
    "                                        \"Test Recall\" : test_class_results[i][\"recall\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "    summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af63e9",
   "metadata": {},
   "source": [
    "## 4. The skeleton of the model training and validation program\n",
    "\n",
    "Your main program should have more or less the following sections and control flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "model = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451dda87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "      (1): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
      "      )\n",
      "      (2): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
      "      )\n",
      "      (1): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
      "      )\n",
      "      (2): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
      "      )\n",
      "      (1): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
      "      )\n",
      "      (2): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
      "      )\n",
      "      (3): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
      "      )\n",
      "      (4): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
      "      )\n",
      "      (5): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
      "      )\n",
      "      (6): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
      "      )\n",
      "      (7): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
      "      )\n",
      "      (8): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
      "      )\n",
      "      (1): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
      "      )\n",
      "      (2): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# import statements for python, torch and companion libraries and your own modules\n",
    "\n",
    "\n",
    "#besion d'un sigmoide \n",
    "# et pci lost partie 4\n",
    "\n",
    "\n",
    "\n",
    "# device initialization\n",
    "\n",
    "# data directories initialization\n",
    "\n",
    "# instantiation of transforms, datasets and data loaders\n",
    "# TIP : use torch.utils.data.random_split to split the training set into train and validation subsets\n",
    "\n",
    "# class definitions\n",
    "\n",
    "# instantiation and preparation of network model\n",
    "\n",
    "# instantiation of loss criterion\n",
    "# instantiation of optimizer, registration of network parameters\n",
    "\n",
    "# definition of current best model path\n",
    "# initialization of model selection metric\n",
    "\n",
    "# creation of tensorboard SummaryWriter (optional)\n",
    "\n",
    "# epochs loop:\n",
    "#   train\n",
    "#   validate on train set\n",
    "#   validate on validation set\n",
    "#   update graphs (optional)\n",
    "#   is new model better than current model ?\n",
    "#       save it, update current best metric\n",
    "\n",
    "# close tensorboard SummaryWriter if created (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4855a7",
   "metadata": {},
   "source": [
    "## 5. The skeleton of the test submission program\n",
    "\n",
    "This, much simpler, program should have the following sections and control flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements for python, torch and companion libraries and your own modules\n",
    "# TIP: use the python standard json module to write python dictionaries as JSON files\n",
    "\n",
    "# global variables defining inference hyper-parameters among other things \n",
    "# DON'T forget the multi-task classification probability threshold\n",
    "\n",
    "# data, trained model and output directories/filenames initialization\n",
    "\n",
    "# device initialization\n",
    "\n",
    "# instantiation of transforms, dataset and data loader\n",
    "\n",
    "# load network model from saved file\n",
    "\n",
    "# initialize output dictionary\n",
    "\n",
    "# prediction loop over test_loader\n",
    "#    get mini-batch\n",
    "#    compute network output\n",
    "#    threshold network output\n",
    "#    update dictionary entries write corresponding class indices\n",
    "\n",
    "# write JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8106c",
   "metadata": {},
   "source": [
    "## 6. Our Approach\n",
    "\n",
    "The purpose of the task was, to our eyes, to find the best \"good enough\" model and parameters settings to answer to te question of multi-label classification for this specific dataset. We had the freedom to chose from a large list of pre-trained models, available in the PyTorch library, to help us with our task. \n",
    "\n",
    "This task alone is very time consuming: it requires testing the classification by modifying a couple of parameters, all while also testing between different models to see which one is the best fitted. We understood we were gonna have to make some decisions if we wanted to optimize our work. Our approach was then the following:\n",
    "\n",
    "- choosing a couple of models from the PyTorch list \n",
    "- choosing a couple of parameters that we would modify along the testing phase\n",
    "\n",
    "It was very important for us to set these variables, if not, the work would have been too long to produce. Also, chosing the \"best\" model is impossible for us, due to an obvious lack of ressources and time.\n",
    "\n",
    "We eventually decided to choose the models based on an energy focus. We know how much energy cand resource consuming is the training of AI models nowadays. This pressing concern is largely discussed and raises questions about the ethical use of these tools. It made us think of trying to find the best working model out of the most reduced ones, that is, those with the less parameters, all for a purpose of saving time and ressources. We understood that by taking this decision, our final model would perform worse once updated to the leaderboard comapared to other, bigger models. We still decided to search between the couple rather small models in the PyTorch distribution, trying to come up with the one that is \"good enough\". The models we decided to test are the following:\n",
    "\n",
    "- MobileNetV3-Small\n",
    "- ResNet-18\n",
    "\n",
    "We deliberately focused on these two architectures because they are not only computationally efficient but also well suited to multi-label image classification from an architectural standpoint. Also because we studied them in class (apart from EfficientNet, which was just mentioned). They have a small amount of parameters compared to other architectures. \n",
    "\n",
    "MobileNetV3-Small was designed specifically for low-resource environments. Architecturally, it combines depthwise separable convolutions (greatly reducing the number of parameters and multiplications) with inverted residual blocks and linear bottlenecks, which mantain representational power while keeping the model extremely light. It also integrates squeeze-and-excitation attention blocks, improving the network’s ability to focus on informative channels, something important in multi-label problems where several objects may share the image. Also, its small parameter count makes it fast to train and low on memory and energy cost. It has 3 million parameters.\n",
    "\n",
    "On the other hand, ResNet-18 is a more classical but still efficient architecture that we studied in class. Its residual skip connections allow for deeper networks to train without vanishing gradients while still keeping the parameter count moderate compared to very deep ResNets and other models. Its parameter count is in the 11 million order. Although it uses standard convolutions (no depthwise separable trick), its straightforward structure is robust and proven for general image recognition. For a multi-label task, the residual connections help the model learn richer representations without becoming prohibitively heavy.\n",
    "\n",
    "\n",
    "We explicitly avoided architectures such as ConvNeXt, which, while state-of-the-art, are much deeper and more computationally expensive, and models like VGG or DenseNet, which are parameter-heavy with less efficient use of computation. Our three chosen models represent different CNN design evolutions: classic residual learning (ResNet-18), mobile-optimized depthwise convolutions (MobileNetV3), and balanced compound scaling (EfficientNet-B0). This mix gives us a practical space to experiment with accuracy vs. efficiency trade-offs for our dataset while keeping training feasible and energy-aware.\n",
    "\n",
    "We then chose which parameters we would focus on, before diving into the code and then the testing. There are a large number of parameters to experiment with when trying to perfectionate a model. We decided, again for a question of efficiency, to set a couple of them (the most impactufl) so we could make an honest study, avoiding changing of parameters between models. These are the parameters, which we divided into two sections:\n",
    "\n",
    "- efficiency parameters:\n",
    "    - number of cpus\n",
    "    - batch size\n",
    "\n",
    "- learning parameters:\n",
    "    - number of epochs\n",
    "    - learning rate\n",
    "    - image size\n",
    "    - optimizer\n",
    "    - weight decay\n",
    "    - dropout layer\n",
    "    - threshold\n",
    "\n",
    "These are the most important parameters in terms of impact in a model's behaviour. There is a larger number of them, but we don't have the time to make a big testing pool with all of them.\n",
    "\n",
    "We then drew the lines for what would be our attack. We first set up our working environment, that is, setting up the 'Github' page for our code and a shared 'Drive' for our results (excel and word). We then needed to make a 'main.py' code for the training of our models, and a 'test.py' for the generation of the JSON file we would eventually upload to the leaderboard. The main modeules of the code were already given in this notebook. Then, each choosing one model, we would test its perfomance while modifying its parameters to find the best working one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9353908",
   "metadata": {},
   "source": [
    "## 7. The Code\n",
    "\n",
    "### 7.1 Custom datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994c6e0",
   "metadata": {},
   "source": [
    "### 7.2 Custom datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9a80d",
   "metadata": {},
   "source": [
    "### 7.3 Training and Validation loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb146ff5",
   "metadata": {},
   "source": [
    "### 7.4 Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57de9ad",
   "metadata": {},
   "source": [
    "### 7.5 Model training and validation program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e22cda6",
   "metadata": {},
   "source": [
    "### 7.6 Test submission program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b109d61",
   "metadata": {},
   "source": [
    "## 8. Results and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952d71a",
   "metadata": {},
   "source": [
    "### 8.1 MobileNetV3-Small\n",
    "\n",
    "We decided not to freeze the layer parameters for the MobileNet model, since it is already a fatst model, and we didn't want to lose any precision.\n",
    "\n",
    "#### 8.1.1 1st run, 2nd and 3rd run\n",
    "\n",
    "The first runs with MobileNet were initially to check its initial performance and time of training. We didn't even run them through the 'test.py' script, since we mainly wanted to take a glance at which were going to be the best efficiency parameters. \n",
    "\n",
    "<div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 10px;\">\n",
    "  <div><b>mobilenet_v3_small run 1</b><br>number of cpus: 12<br>batch size: 32<br>number epochs: 5<br>learning rate: 0.001<br>image size: 224<br>optimizer: adam<br>weight decay: no<br>dropout layer: no<br>threshold: 0.5<br>best F1: 0.3585<br>Time (min): 17</div>\n",
    "</div>\n",
    "\n",
    "We used the following values for the parameters (randome but coherent ones). We realized the time it took was in accordance to what we expected from a small model. So was the best F1 value, which was not the highest but was still good. Just not good enough yet. \n",
    "\n",
    "Between run 1 and 2, we only changed the number of epochs from 5 to 8 to see the impact on the Time and F1 score. Time increased to 20 minutes, and F1 score stayed the same. We decided to keep increasing the number of epochs to see the changes. We wanted to get to athe point where it was already too long, even for a small model. We tried 15 next, and realised that F1 was now 0,38, but it stayed relatively constant after epoch number 10. We finally decided to keep epochs at 10 for the moment. It was time now to test the learning parameters to try and maximize the F1 score and other metrics. \n",
    "\n",
    "![Alt text](images/1.png)\n",
    "\n",
    "Loss on validation set pikes after 10 epochs\n",
    "\n",
    "<div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 10px;\">\n",
    "  <div><b>mobilenet_v3_small run 2</b><br>number of cpus: 12<br>batch size: 32<br>number epochs: 8<br>learning rate: 0.001<br>image size: 224<br>optimizer: adam<br>weight decay: no<br>dropout layer: no<br>threshold: 0.5<br>best F1: 0.3555<br>Time (min): 20</div>\n",
    "  <div><b>mobilenet_v3_small run 3</b><br>number of cpus: 12<br>batch size: 64<br>number epochs: 15<br>learning rate: 0.001<br>image size: 224<br>optimizer: adam<br>weight decay: no<br>dropout layer: no<br>threshold: 0.5<br>best F1: 0.3786<br>Time (min): 33</div>\n",
    "</div>\n",
    "\n",
    "For run 3, we also tried to change the batch size to increase it. This was made in order to accelerate the running time, batch size being the number of samples processed before one gradient update. Indeed, with a small batch size, the training can be slower to converge. With a big batch size, training converges faster, but we lose in generalization and can fall into overfitting because there is less gradient noise.\n",
    "\n",
    "\n",
    "#### 8.1.2 4th and 5th run\n",
    "\n",
    "Even if our F1 score got better with run 3, we observed that the model was failing to generalize, maybe due to the batch size having increased. \n",
    "\n",
    "![Alt text](images/2.png)\n",
    "![Alt text](images/3.png)\n",
    "![Alt text](images/4.png)\n",
    "\n",
    "See the gap between the train and validation accuracy metric, and the rapid stagnation of precision and therefore wrong labelisation in the validation set. This was a sign of the model generalizing to us. This usually means the model is predicting more classes as positive over time (hence the recall increases), but among those extra predictions, more are false positives, therefore precision stops improving and, in this case, gets worse. The model gets more confident and predicts more positives, but it also adds noise.\n",
    "\n",
    "However, we wanted to keep our model fast, so we tried methods to better our generalization other than reducing the batch size. \n",
    "\n",
    "For run 4, we chose to change our optimizer from 'adam' to 'adamw', which presents better regularization from decoupling its weight decay. We also reduced the learning rate and increased the image size, more as a way of testing these parameters effects. We would later find out increasing image size helps the model to see more in detail, since the dimension of the images in the set are of bigger resolution, and therefore contributes to overfitting. Indeed, our precision metric for the training set was too high. Same for learning rate, since a smaller value lets the optimizer make finer updates instead of big jumps.\n",
    "\n",
    "![Alt text](images/5.png)\n",
    "\n",
    "We could see in run 4 that setting our epochs to 10 had helped with stabilizing the validation loss. However, as we can see in the curbs below, the gaps between training and validation metrics were still present, and the precision, even if of much better value, was still stagnating and therefore the model was still overfitting. \n",
    "\n",
    "\n",
    "![Alt text](images/6.png)\n",
    "![Alt text](images/7.png)\n",
    "\n",
    "The final F1 metric did not vary.\n",
    "\n",
    "<div style=\"display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px;\">\n",
    "  <div><b>mobilenet_v3_small run 4</b><br>number of cpus: 14<br>batch size: 64<br>number epochs: 10<br>learning rate: 0.0001<br>image size: 256<br>optimizer: adamw<br>weight decay: 0.0001<br>dropout layer: no<br>threshold: 0.5<br>best F1: 0.3814<br>Time (min): 42</div>\n",
    "</div>\n",
    "\n",
    "Results were similar for run number 5\n",
    "\n",
    "We tried to counter overfitting by setting a higher learning rate than before. Also, we decided to increase the weight decay for 'adamW' and adding a dropout phase to randomly disable neurons during training so the network can’t rely on specific paths, forcing it to learn more robust features and generalize better.\n",
    "\n",
    "Results were almost identical to those of run 4, with validation precision rising fast and then flattening while recall slowly increases. The model was clearly becoming more “liberal”, trying to catch more positives but producing more false positives. This is very common on COCO (high class imbalance, many small objects).\n",
    "\n",
    "<div style=\"display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px;\">\n",
    "  <div><b>mobilenet_v3_small run 5</b><br>number of cpus: 14<br>batch size: 64<br>number epochs: 10<br>learning rate: 0.0003<br>image size: 256<br>optimizer: adamw<br>weight decay: 0.0003<br>dropout layer: yes<br>threshold: 0.5<br>best F1: 0.38<br>Time (min): 45</div>\n",
    "</div>\n",
    "\n",
    "This time, we decided to test the best model chosen in the run on the 'test.py', in order to upload the JSON file to the leaderboard. We were surprised to see the F1 metric to be equal to 0.4227 which meant it was doing better than in the validation set, which we thought was unusual. \n",
    "\n",
    "#### 8.1.3 6th and 7th run \n",
    "\n",
    "For run number 6 we clearly needed a more agressive generalizing approach. We set up a large number of epochs to see if that was the problem, all while increasing the weight decay once again. We also tried other transforms to make the set more random, such as 'ColorJitter'. At the moment we only did 'RandomHorizontalFlip'.\n",
    "\n",
    "The F1 metric for the validation set improved a lot, but we concluded it was just a question of a high epoch number. Indeed, the model still overfitted, as we tried it on the 'test.py' and it performed worse than the one before. The curbs also showed this trend. Precision was again falling, this time similarly to run 3 (plummeting after 10 epochs). The gaps were still seen in the other metrics between the validation and testing models, but our main concern was the precision metric. \n",
    "\n",
    "When testing in the test set, the F1 metric was 0.4145, which was worse than the previous run's one. Also, the time to complete the training was too long if we wanted to behave according to our initial intent of an environment friendly model.\n",
    "\n",
    "<div style=\"display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px;\">\n",
    "  <div><b>mobilenet_v3_small run 6</b><br>number of cpus: 14<br>batch size: 64<br>number epochs: 20<br>learning rate: 0.0003<br>image size: 256<br>optimizer: adamw<br>weight decay: 0.001<br>dropout layer: yes<br>threshold: 0.5</div>\n",
    "</div>\n",
    "\n",
    "The number of epochs was not the problem, as stagnation of the precision happened way too early in the training process. We later came back to a more reduced number of epochs, also to try to meet our initial approach of a environment friendly model. \n",
    "\n",
    "For run number 7, we came back to little epochs. We also tried another technique, which was tuning the decision threshold to boost precision. We changed the code to integrate this feature. In our experiments, the network outputs a probability for each possible label. By default, a label is considered present if its probability is ≥ 0,5. However, this fixed value does not necessarily mean the best trade-off between precision (proportion of predicted positives that are correct) and recall (proportion of true positives that are detected). We therefore performed a threshold sweep, testing several cut-off values from 0.05 to 0.95 on the validation set. Increasing the threshold makes the model predict a label only when it is more confident, which reduces false positives and generally improves precision, although it can slightly decrease recall. Conversely, lowering the threshold increases recall but often introduces more false positives, lowering precision. Selecting the threshold that maximized the F1-score (the harmonic mean of precision and recall) allowed us to improve our model’s validation performance without retraining. This simple post-processing step significantly increased precision and produced a better overall F1 balance for the multi-label task.\n",
    "\n",
    "We also reduced the batch size again and the image size. Our F1 improved slightly from first runs, and specially, our precision on validation doesn't present the same gap and evolution as before. We believe the sweeping threshold and the reducing of the batch size allowed us to avoid generalization from the model. These are our curbs for run number 7:\n",
    "\n",
    "![Alt text](images/8.png)\n",
    "\n",
    "We can see that even if the F1 metric is not so much better, the precision improved from the first runs, all while using a small model (5mins/epoch). \n",
    "\n",
    "![Alt text](images/9.png)\n",
    "\n",
    "The accuracy and F1 metrics don't seem to stagnate now. We could make it run for more epochs, but it would diverge from our main objective in this task. We want to keep the model short and efficient. \n",
    "\n",
    "<div style=\"display: grid; grid-template-columns: 1fr; gap: 10px;\">\n",
    "  <div><b>mobilenet_v3_small run 7</b><br>number of cpus: 14<br>batch size: 32<br>number epochs: 15<br>learning rate: 0.0001<br>image size: 224<br>optimizer: adamw<br>weight decay: 0.001<br>dropout layer: yes<br>threshold: sweeping<br>best F1: 0.3971<br>Time (min): 69</div>\n",
    "</div>\n",
    "\n",
    "\n",
    "When testing on 'test.py', the final F1 metric is not that impressive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e282bc3",
   "metadata": {},
   "source": [
    "### Resnet 18\n",
    "#### Intro\n",
    "\n",
    "ResNet-18 is a deep learning model with 18 layers and about 11.7M weights. It uses skip connections to train more effectively.\n",
    "\n",
    "We use it with the MS COCO dataset because it’s lightweight, fast, and still powerful enough to learn useful features from COCO’s large variety of images and objects.\n",
    "\n",
    "To train this model we changed computeur to also test training with a faster computeur. We use an  an NVIDIA RTX A2000 8GB and I7 CPU\n",
    "#### Why we chose ResNet-18\n",
    "\n",
    "\n",
    "Efficiency: ResNet-18 is computationally less demanding compared to deeper models, making it suitable for tasks requiring faster inference times. That means less Time to wait and less power consumtion it's a main value for us.\n",
    "\n",
    "Transfer Learning: Pretrained ResNet-18 models on ImageNet can be fine-tuned for COCO, leveraging learned features for improved performance.\n",
    "\n",
    "Proven Architecture: ResNet architectures, including ResNet-18, have demonstrated strong performance in various vision tasks, including classification and detection.\n",
    "\n",
    "\n",
    "\n",
    "#### Let's Run it\n",
    "##### 1\n",
    "For the first run we will use the parameters used in mobileNet to have a base line and see what we can improve afterwars.\n",
    "\n",
    "On the first run of ResNet18, we didn’t freeze any model layers. For this initial test, we trained for 2 hours and achieved an F1 score of 0.42 after 10 epochs. The loss was still decreasing, so extending the number of epochs in the next run may improve results. We used a learning rate of 1e-4; increasing it to 1e-3 could allow for faster convergence.\n",
    "\n",
    "![Alt text](images/Metrics-Resnet18-1.png)\n",
    "##### 2\n",
    "\n",
    "\n",
    "After a run freezing the backbone, we obtained an F1 score of 0.46. However, when training for 11 epochs, we achieved a better F1 score of 0.51. We also observed overfitting, accompanied by a decrease in accuracy. Therefore, we decided to reduce the number of epochs to 10, add a dropout layer of 0.4, and resize the images to 224. According to the ResNet-18 paper, this is the input size on which the network was originally trained, which can lead to better results through improved contextualization.\n",
    "The run with the dropout layer gave us an F1 score of 0.010 after 3 epochs so we decided to stop it.\n",
    "\n",
    "##### 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d96198",
   "metadata": {},
   "source": [
    "## 9. To go further\n",
    "\n",
    "We decided as a future case of study to try other models results on this task. We tried EfficientNet-B0 and ConvNext.\n",
    "\n",
    "EfficientNet-B0 is built around the idea of compound scaling, where network depth, width, and input resolution are balanced using a single scaling coefficient. This avoids over-widening or over-deepening the network unnecessarily. Architecturally, it uses mobile inverted bottleneck convolution blocks with SE attention like MobileNetV3, but with a  deeper design that improves feature extraction while staying efficient. This is great for multi-label classification. It is a good balance between low computational work and big number of parameters.\n",
    "\n",
    "ConvNeXt is a modern reinterpretation of the classic ResNet architecture, redesigned with ideas borrowed from Vision Transformers while keeping a fully convolutional backbone. It simplifies the traditional residual blocks into depthwise separable convolutions with large kernels (7×7), layer normalization instead of batch normalization, and inverted bottlenecks similar to those in efficient mobile models. This design improves the receptive field and feature extraction capacity while remaining computationally efficient compared to older CNNs. It is however a big model with a large number of parameters and a large computional time, which derives from our main approach to the task. \n",
    "\n",
    "We just wanted to compare our results with those of other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ff4cfe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bfae342",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
