{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98da5ac3",
   "metadata": {},
   "source": [
    "# The MS COCO classification challenge\n",
    "\n",
    "Razmig KÃ©chichian\n",
    "\n",
    "This notebook defines the multi-class classification challenge on the [MS COCO dataset](https://cocodataset.org/). It defines the problem, sets the rules of organization and presents tools you are provided with to accomplish the challenge.\n",
    "\n",
    "\n",
    "## 1. Problem statement\n",
    "\n",
    "Each image has **several** categories of objects to predict, hence the difference compared to the classification problem we have seen on the CIFAR10 dataset where each image belonged to a **single** category, therefore the network loss function and prediction mechanism (only highest output probability) were defined taking this constraint into account.\n",
    "\n",
    "We adapted the MS COCO dataset for the requirements of this challenge by, among other things, reducing the number of images and their dimensions to facilitate processing.\n",
    "\n",
    "In the companion `ms-coco.zip` compressed directory you will find two sub-directories:\n",
    "- `images`: which contains the images in train (65k) and test (~5k) subsets,\n",
    "- `labels`: which lists labels for each of the images in the train subset only.\n",
    "\n",
    "Each label file gives a list of class IDs that correspond to the class index in the following tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \n",
    "           \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
    "           \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",       \n",
    "           \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "           \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "           \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \n",
    "           \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \n",
    "           \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \n",
    "           \"hair drier\", \"toothbrush\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf52f93",
   "metadata": {},
   "source": [
    "Your goal is to follow a **transfer learning strategy** in training and validating a network on **your own distribution of training data into training and a validation subsets**, then to **test it on the test subset** by producing a [JSON file](https://en.wikipedia.org/wiki/JSON) with content of the following format:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"000000000139\": [\n",
    "        56,\n",
    "        60,\n",
    "        62\n",
    "    ],\n",
    "    \"000000000285\": [\n",
    "        21,\n",
    "    ],\n",
    "    \"000000000632\": [\n",
    "        57,\n",
    "        59,\n",
    "    73\n",
    "    ],\n",
    "    # other test images\n",
    "}\n",
    "```\n",
    "\n",
    "In this file, the name (without extension) of each test image is associated with a list of class indices predicted by your network. Make sure that the JSON file you produce **follows this format strictly**.\n",
    "\n",
    "You will submit your JSON prediction file to the following [online evaluation server and leaderboard](https://www.creatis.insa-lyon.fr/kechichian/ms-coco-classif-leaderboard.html), which will evaluate your predictions on test set labels, unavailable to you.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>WARNING:</b> Use this server with <b>the greatest care</b>. A new submission with identical Participant or group name will <b>overwrite</b> the identically named submission, if one already exists, therefore check the leaderboard first. <b>Do not make duplicate leaderboard entries for your group</b>, keep track of your test scores privately. Also pay attention to upload only JSON files of the required format.<br>\n",
    "</div>\n",
    "\n",
    "The evaluation server calculates and returns mean performances over all classes, and optionally per class performances. Entries in the leaderboard are sorted by the F1 metric.\n",
    "\n",
    "You can request an evaluation as many times as you want. It is up to you to specify the final evaluation by updating the leaderboard entry corresponding to your Participant or group name. This entry will be taken into account for grading your work.\n",
    "\n",
    "It goes without saying that it is **prohibited** to use another distribution of the MS COCO database for training, e.g. the Torchvision dataset.\n",
    "\n",
    "\n",
    "## 2. Organization\n",
    "\n",
    "- Given the scope of the project, you will work in groups of 2. \n",
    "- Work on the challenge begins on IAV lab 3 session, that is on the **23rd of September**.\n",
    "- Results are due 10 days later, that is on the **3rd of October, 18:00**. They comrpise:\n",
    "    - a submission to the leaderboard,\n",
    "    - a commented Python script (with any necessary modules) or Jupyter Notebook, uploaded on Moodle in the challenge repository by one of the members of the group.\n",
    "    \n",
    "    \n",
    "## 3. Tools\n",
    "\n",
    "In addition to the MS COCO annotated data and the evaluation server, we provide you with most code building blocks. Your task is to understand them and use them to create the glue logic, that is the main program, putting all these blocks together and completing them as necessary to implement a complete machine learning workflow to train and validate a model, and produce the test JSON file.\n",
    "\n",
    "### 3.1 Custom `Dataset`s\n",
    "\n",
    "We provide you with two custom `torch.utils.data.Dataset` sub-classes to use in training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class COCOTrainImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, annotations_dir, max_images=None, transform=None):\n",
    "        self.img_labels = sorted(glob(\"*.cls\", root_dir=annotations_dir))\n",
    "        if max_images:\n",
    "            self.img_labels = self.img_labels[:max_images]\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, Path(self.img_labels[idx]).stem + \".jpg\")\n",
    "        labels_path = os.path.join(self.annotations_dir, self.img_labels[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        with open(labels_path) as f: \n",
    "            labels = [int(label) for label in f.readlines()]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        labels = torch.zeros(80).scatter_(0, torch.tensor(labels), value=1)\n",
    "        return image, labels\n",
    "\n",
    "\n",
    "class COCOTestImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_list = sorted(glob(\"*.jpg\", root_dir=img_dir))    \n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_list[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, Path(img_path).stem # filename w/o extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d805e2",
   "metadata": {},
   "source": [
    "### 3.2 Training and validation loops\n",
    "\n",
    "The following are two general-purpose classification train and validation loop functions to be called inside the epochs for-loop with appropriate argument settings.\n",
    "\n",
    "Pay particular attention to the `validation_loop()` function's arguments `multi_task`, `th_multi_task` and `one_hot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb693462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def train_loop(train_loader, net, criterion, optimizer, device,\n",
    "               mbatch_loss_group=-1):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    mbatch_losses = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # following condition False by default, unless mbatch_loss_group > 0\n",
    "        if i % mbatch_loss_group == mbatch_loss_group - 1:\n",
    "            mbatch_losses.append(running_loss / mbatch_loss_group)\n",
    "            running_loss = 0.0\n",
    "    if mbatch_loss_group > 0:\n",
    "        return mbatch_losses\n",
    "\n",
    "\n",
    "def validation_loop(val_loader, net, criterion, num_classes, device,\n",
    "                    multi_task=False, th_multi_task=0.5, one_hot=False, class_metrics=False):\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    size = len(val_loader.dataset)\n",
    "    class_total = {label:0 for label in range(num_classes)}\n",
    "    class_tp = {label:0 for label in range(num_classes)}\n",
    "    class_fp = {label:0 for label in range(num_classes)}\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item() * images.size(0)\n",
    "            if not multi_task:    \n",
    "                predictions = torch.zeros_like(outputs)\n",
    "                predictions[torch.arange(outputs.shape[0]), torch.argmax(outputs, dim=1)] = 1.0\n",
    "            else:\n",
    "                predictions = torch.where(outputs > th_multi_task, 1.0, 0.0)\n",
    "            if not one_hot:\n",
    "                labels_mat = torch.zeros_like(outputs)\n",
    "                labels_mat[torch.arange(outputs.shape[0]), labels] = 1.0\n",
    "                labels = labels_mat\n",
    "                \n",
    "            tps = predictions * labels\n",
    "            fps = predictions - tps\n",
    "            \n",
    "            tps = tps.sum(dim=0)\n",
    "            fps = fps.sum(dim=0)\n",
    "            lbls = labels.sum(dim=0)  \n",
    "                \n",
    "            for c in range(num_classes):\n",
    "                class_tp[c] += tps[c]\n",
    "                class_fp[c] += fps[c]\n",
    "                class_total[c] += lbls[c]\n",
    "                    \n",
    "            correct += tps.sum()\n",
    "\n",
    "    class_prec = []\n",
    "    class_recall = []\n",
    "    freqs = []\n",
    "    for c in range(num_classes):\n",
    "        class_prec.append(0 if class_tp[c] == 0 else\n",
    "                          class_tp[c] / (class_tp[c] + class_fp[c]))\n",
    "        class_recall.append(0 if class_tp[c] == 0 else\n",
    "                            class_tp[c] / class_total[c])\n",
    "        freqs.append(class_total[c])\n",
    "\n",
    "    freqs = torch.tensor(freqs)\n",
    "    class_weights = 1. / freqs\n",
    "    class_weights /= class_weights.sum()\n",
    "    class_prec = torch.tensor(class_prec)\n",
    "    class_recall = torch.tensor(class_recall)\n",
    "    prec = (class_prec * class_weights).sum()\n",
    "    recall = (class_recall * class_weights).sum()\n",
    "    f1 = 2. / (1/prec + 1/recall)\n",
    "    val_loss = loss / size\n",
    "    accuracy = correct / freqs.sum()\n",
    "    results = {\"loss\": val_loss, \"accuracy\": accuracy, \"f1\": f1,\\\n",
    "               \"precision\": prec, \"recall\": recall}\n",
    "\n",
    "    if class_metrics:\n",
    "        class_results = []\n",
    "        for p, r in zip(class_prec, class_recall):\n",
    "            f1 = (0 if p == r == 0 else 2. / (1/p + 1/r))\n",
    "            class_results.append({\"f1\": f1, \"precision\": p, \"recall\": r})\n",
    "        results = results, class_results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d8e84",
   "metadata": {},
   "source": [
    "### 3.3 Tensorboard logging (optional)\n",
    "\n",
    "Evaluation metrics and losses produced by the `validation_loop()` function on train and validation data can be logged to a [Tensorboard `SummaryWriter`](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) which allows you to observe training graphically via the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graphs(summary_writer, epoch, train_results, test_results,\n",
    "                  train_class_results=None, test_class_results=None, \n",
    "                  class_names = None, mbatch_group=-1, mbatch_count=0, mbatch_losses=None):\n",
    "    if mbatch_group > 0:\n",
    "        for i in range(len(mbatch_losses)):\n",
    "            summary_writer.add_scalar(\"Losses/Train mini-batches\",\n",
    "                                  mbatch_losses[i],\n",
    "                                  epoch * mbatch_count + (i+1)*mbatch_group)\n",
    "\n",
    "    summary_writer.add_scalars(\"Losses/Train Loss vs Test Loss\",\n",
    "                               {\"Train Loss\" : train_results[\"loss\"],\n",
    "                                \"Test Loss\" : test_results[\"loss\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Accuracy vs Test Accuracy\",\n",
    "                               {\"Train Accuracy\" : train_results[\"accuracy\"],\n",
    "                                \"Test Accuracy\" : test_results[\"accuracy\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train F1 vs Test F1\",\n",
    "                               {\"Train F1\" : train_results[\"f1\"],\n",
    "                                \"Test F1\" : test_results[\"f1\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Precision vs Test Precision\",\n",
    "                               {\"Train Precision\" : train_results[\"precision\"],\n",
    "                                \"Test Precision\" : test_results[\"precision\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    summary_writer.add_scalars(\"Metrics/Train Recall vs Test Recall\",\n",
    "                               {\"Train Recall\" : train_results[\"recall\"],\n",
    "                                \"Test Recall\" : test_results[\"recall\"]},\n",
    "                               (epoch + 1) if not mbatch_group > 0\n",
    "                                     else (epoch + 1) * mbatch_count)\n",
    "\n",
    "    if train_class_results and test_class_results:\n",
    "        for i in range(len(train_class_results)):\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train F1 vs Test F1\",\n",
    "                                       {\"Train F1\" : train_class_results[i][\"f1\"],\n",
    "                                        \"Test F1\" : test_class_results[i][\"f1\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train Precision vs Test Precision\",\n",
    "                                       {\"Train Precision\" : train_class_results[i][\"precision\"],\n",
    "                                        \"Test Precision\" : test_class_results[i][\"precision\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "\n",
    "            summary_writer.add_scalars(f\"Class Metrics/{class_names[i]}/Train Recall vs Test Recall\",\n",
    "                                       {\"Train Recall\" : train_class_results[i][\"recall\"],\n",
    "                                        \"Test Recall\" : test_class_results[i][\"recall\"]},\n",
    "                                       (epoch + 1) if not mbatch_group > 0\n",
    "                                             else (epoch + 1) * mbatch_count)\n",
    "    summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af63e9",
   "metadata": {},
   "source": [
    "## 4. The skeleton of the model training and validation program\n",
    "\n",
    "Your main program should have more or less the following sections and control flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "model = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451dda87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "      (1): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
      "      )\n",
      "      (2): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
      "      )\n",
      "      (1): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
      "      )\n",
      "      (2): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
      "      )\n",
      "      (1): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
      "      )\n",
      "      (2): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
      "      )\n",
      "      (3): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
      "      )\n",
      "      (4): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
      "      )\n",
      "      (5): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
      "      )\n",
      "      (6): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
      "      )\n",
      "      (7): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
      "      )\n",
      "      (8): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
      "      )\n",
      "      (1): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
      "      )\n",
      "      (2): CNBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (4): GELU(approximate='none')\n",
      "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (6): Permute()\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# import statements for python, torch and companion libraries and your own modules\n",
    "\n",
    "\n",
    "#besion d'un sigmoide \n",
    "# et pci lost partie 4\n",
    "\n",
    "\n",
    "\n",
    "# device initialization\n",
    "\n",
    "# data directories initialization\n",
    "\n",
    "# instantiation of transforms, datasets and data loaders\n",
    "# TIP : use torch.utils.data.random_split to split the training set into train and validation subsets\n",
    "\n",
    "# class definitions\n",
    "\n",
    "# instantiation and preparation of network model\n",
    "\n",
    "# instantiation of loss criterion\n",
    "# instantiation of optimizer, registration of network parameters\n",
    "\n",
    "# definition of current best model path\n",
    "# initialization of model selection metric\n",
    "\n",
    "# creation of tensorboard SummaryWriter (optional)\n",
    "\n",
    "# epochs loop:\n",
    "#   train\n",
    "#   validate on train set\n",
    "#   validate on validation set\n",
    "#   update graphs (optional)\n",
    "#   is new model better than current model ?\n",
    "#       save it, update current best metric\n",
    "\n",
    "# close tensorboard SummaryWriter if created (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4855a7",
   "metadata": {},
   "source": [
    "## 5. The skeleton of the test submission program\n",
    "\n",
    "This, much simpler, program should have the following sections and control flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements for python, torch and companion libraries and your own modules\n",
    "# TIP: use the python standard json module to write python dictionaries as JSON files\n",
    "\n",
    "# global variables defining inference hyper-parameters among other things \n",
    "# DON'T forget the multi-task classification probability threshold\n",
    "\n",
    "# data, trained model and output directories/filenames initialization\n",
    "\n",
    "# device initialization\n",
    "\n",
    "# instantiation of transforms, dataset and data loader\n",
    "\n",
    "# load network model from saved file\n",
    "\n",
    "# initialize output dictionary\n",
    "\n",
    "# prediction loop over test_loader\n",
    "#    get mini-batch\n",
    "#    compute network output\n",
    "#    threshold network output\n",
    "#    update dictionary entries write corresponding class indices\n",
    "\n",
    "# write JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8106c",
   "metadata": {},
   "source": [
    "## 6. Our Approach\n",
    "\n",
    "The purpose of the task was, to our eyes, to find the best \"good enough\" model and parameters settings to answer to te question of multi-label classification for this specific dataset. We had the freedom to chose from a large list of pre-trained models, available in the PyTorch library, to help us with our task. \n",
    "\n",
    "This task alone is very time consuming: it requires testing the classification by modifying a couple of parameters, all while also testing between different models to see which one is the best fitted. We understood we were gonna have to make some decisions if we wanted to optimize our work. Our approach was then the following:\n",
    "\n",
    "- choosing a couple of models from the PyTorch list \n",
    "- choosing a couple of parameters that we would modify along the testing phase\n",
    "\n",
    "It was very important for us to set these variables, if not, the work would have been too long to produce. Also, chosing the \"best\" model is impossible for us, due to an obvious lack of ressources and time.\n",
    "\n",
    "We eventually decided to choose the models based on an energy focus. We know how much energy cand resource consuming is the training of AI models nowadays. This pressing concern is largely discussed and raises questions about the ethical use of these tools. It made us think of trying to find the best working model out of the most reduced ones, that is, those with the less parameters, all for a purpose of saving time and ressources. We understood that by taking this decision, our final model would perform worse once updated to the leaderboard comapared to other, bigger models. We still decided to search between the couple rather small models in the PyTorch distribution, trying to come up with the one that is \"good enough\". The models we decided to test are the following:\n",
    "\n",
    "- \n",
    "\n",
    "We deliberately focused on three architectures â MobileNetV3-Small, EfficientNet-B0, and ResNet-18 â because they are not only computationally efficient but also well suited to multi-label image classification from an architectural standpoint.\n",
    "\n",
    "MobileNetV3-Small was designed specifically for low-resource environments. Architecturally, it combines depthwise separable convolutions (greatly reducing the number of parameters and multiplications) with inverted residual blocks and linear bottlenecks, which preserve representational power while keeping the model extremely light. It also integrates squeeze-and-excitation (SE) attention blocks, improving the networkâs ability to focus on informative channels â something important in multi-label problems where several objects may share the image. Its small parameter count makes it fast to train and low on memory and energy cost.\n",
    "\n",
    "EfficientNet-B0 is built around the idea of compound scaling, where network depth, width, and input resolution are balanced using a single scaling coefficient. This avoids over-widening or over-deepening the network unnecessarily. Architecturally, it uses mobile inverted bottleneck convolution (MBConv) blocks with SE attention like MobileNetV3, but with a slightly deeper and wider design that improves feature extraction while staying efficient. For multi-label classification, this balance often helps capture both low-level textures and higher-level object cues without huge computational cost.\n",
    "\n",
    "ResNet-18 represents a more classical but still efficient architecture. Its residual skip connections allow for deeper networks to train without vanishing gradients while still keeping the parameter count moderate compared to very deep ResNets. Although it uses standard convolutions (no depthwise separable trick), its straightforward structure is robust and proven for general image recognition. For a multi-label task, the residual connections help the model learn richer representations without becoming prohibitively heavy.\n",
    "\n",
    "We explicitly avoided architectures such as ConvNeXt or Swin Transformer, which, while state-of-the-art, are much deeper and more computationally expensive, and models like VGG or DenseNet, which are parameter-heavy with less efficient use of computation. Our three chosen models represent different CNN design evolutions: classic residual learning (ResNet-18), mobile-optimized depthwise convolutions (MobileNetV3), and balanced compound scaling (EfficientNet-B0). This mix gives us a practical space to experiment with accuracy vs. efficiency trade-offs for our dataset while keeping training feasible and energy-aware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce635f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
